{"cells":[{"source":"# Building Chatbots with the OpenAI API and Pinecone\n\n## By: Vatsal Vinay Parikh","metadata":{},"id":"8c8fde4c-9222-4265-b72b-8d7693520250","cell_type":"markdown"},{"source":"In this project, we aim to explore the fascinating world of AI chatbots. We will be using LangChain, OpenAI, and Pinecone vector DB, to build a chatbot capable of learning from the external world using **R**etrieval **A**ugmented **G**eneration (RAG).\n\nWe will be using a dataset sourced from the Llama 2 ArXiv paper and other related papers to help our chatbot answer questions about the latest and greatest in the world of GenAI.\n\nThis project is designed for learners who have a basic understanding of the OpenAI API and Pinecone, as covered in our previous projects. It's a great opportunity for those interested in AI, machine learning, and NLP to get hands-on experience with building a chatbot with RAG.\n\n![rag](rag.png)\n\nBy the end of this project, you will have a functioning chatbot and RAG pipeline that can hold a conversation and provide informative responses based on a knowledge base. This project is a stepping stone towards understanding and building more complex AI systems in the future.","metadata":{},"id":"3e302e1c-4c18-4c44-87fd-ba935c3a0853","cell_type":"markdown"},{"source":"## Setup","metadata":{},"id":"a9274661-8d8c-4cc5-901e-5fc497866b89","cell_type":"markdown"},{"source":"Before we start building our chatbot, we need to install some Python libraries. Here's a brief overview of what each library does:\n\n- **openai**: This is the official OpenAI Python client. We'll use it to interact with the GPT large language model.\n- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone vector DB where we will store our chatbot's knowledge base.\n- **langchain**, **langchain-openai**, **langchain-pinecone**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n- **tiktoken**: This is a library from OpenAI that allows you to count the number of tokens in a text string without making an API call.\n- **datasets**: This library provides a vast array of datasets for machine learning. We'll use it to load our knowledge base for the chatbot.\n\nYou can install these libraries using pip like so:","metadata":{},"id":"2cf847fd-f8f8-49f6-9b43-0eb098239072","cell_type":"markdown"},{"source":"# Install the openai package, locked to version 1.27\n!pip install openai==1.27\n\n# Install the datasets package, locked to version\n!pip install pinecone-client==4.0.0\n\n# Install the langchain package, locked to version 0.1.19\n!pip install langchain==0.1.19\n\n# Install the langchain-openai package, locked to version 0.1.6\n!pip install langchain-openai==0.1.6\n\n# Update the langchain-pinecone package, locked to version 0.1.0\n!pip install langchain-pinecone==0.1.0\n\n# Update the tiktoken package, locked to version 0.7.0\n!pip install tiktoken==0.7.0\n\n# Install the datasets package, locked to version 2.19.1\n!pip install datasets==2.19.1\n\n# Update the typing_extensions package, locked to version 4.11.0\n!pip install typing_extensions==4.11.0","metadata":{"executionCancelledAt":null,"executionTime":27660,"lastExecutedAt":1741916434971,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Install the openai package, locked to version 1.27\n!pip install openai==1.27\n\n# Install the datasets package, locked to version\n!pip install pinecone-client==4.0.0\n\n# Install the langchain package, locked to version 0.1.19\n!pip install langchain==0.1.19\n\n# Install the langchain-openai package, locked to version 0.1.6\n!pip install langchain-openai==0.1.6\n\n# Update the langchain-pinecone package, locked to version 0.1.0\n!pip install langchain-pinecone==0.1.0\n\n# Update the tiktoken package, locked to version 0.7.0\n!pip install tiktoken==0.7.0\n\n# Install the datasets package, locked to version 2.19.1\n!pip install datasets==2.19.1\n\n# Update the typing_extensions package, locked to version 4.11.0\n!pip install typing_extensions==4.11.0","outputsMetadata":{"0":{"height":579,"type":"stream"}},"lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680","collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"f96e3639-1515-47ce-9cab-21b2c8a43c64","cell_type":"code","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting openai==1.27\n  Downloading openai-1.27.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (4.8.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.27) (1.7.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (0.27.2)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (2.7.1)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.27) (4.12.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.27) (1.2.2)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.27) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.27) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.27) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.27) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.27) (0.7.0)\nRequirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.27) (2.18.2)\nDownloading openai-1.27.0-py3-none-any.whl (314 kB)\nInstalling collected packages: openai\n\u001b[33m  WARNING: The script openai is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0mSuccessfully installed openai-1.27.0\nDefaulting to user installation because normal site-packages is not writeable\nCollecting pinecone-client==4.0.0\n  Downloading pinecone_client-4.0.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==4.0.0) (2025.1.31)\nRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==4.0.0) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==4.0.0) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==4.0.0) (2.3.0)\nDownloading pinecone_client-4.0.0-py3-none-any.whl (214 kB)\nInstalling collected packages: pinecone-client\n  Attempting uninstall: pinecone-client\n    Found existing installation: pinecone-client 6.0.0\n    Uninstalling pinecone-client-6.0.0:\n      Successfully uninstalled pinecone-client-6.0.0\nSuccessfully installed pinecone-client-4.0.0\nDefaulting to user installation because normal site-packages is not writeable\nCollecting langchain==0.1.19\n  Downloading langchain-0.1.19-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (2.0.38)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (3.11.12)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (0.6.7)\nRequirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (0.0.38)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (0.1.53)\nRequirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (0.0.2)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (0.1.147)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (2.7.1)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (2.32.3)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.19) (8.5.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.19) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (0.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.19) (1.33)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.19) (23.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (0.27.2)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (3.10.15)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.0.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.19) (0.7.0)\nRequirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.19) (2.18.2)\nRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.19) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.19) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.19) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.19) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.19) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.19) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (4.8.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.0.7)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain==0.1.19) (3.0.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.19) (1.0.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.19) (1.2.2)\nDownloading langchain-0.1.19-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: langchain\n\u001b[33m  WARNING: The script langchain-server is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0mSuccessfully installed langchain-0.1.19\nDefaulting to user installation because normal site-packages is not writeable\nCollecting langchain-openai==0.1.6\n  Downloading langchain_openai-0.1.6-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.46 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.1.6) (0.1.53)\nRequirement already satisfied: openai<2.0.0,>=1.24.0 in /home/repl/.local/lib/python3.10/site-packages (from langchain-openai==0.1.6) (1.27.0)\nRequirement already satisfied: tiktoken<1,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.1.6) (0.7.0)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (6.0.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (0.1.147)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (23.2)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (2.7.1)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (8.5.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (4.8.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (1.7.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (0.27.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (4.12.2)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (2023.12.25)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (2.32.3)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (1.2.2)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.6) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (3.0.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (3.10.15)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (1.0.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (0.7.0)\nRequirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai==0.1.6) (2.18.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai==0.1.6) (2.3.0)\nDownloading langchain_openai-0.1.6-py3-none-any.whl (34 kB)\nInstalling collected packages: langchain-openai\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nembedchain 0.1.113 requires langchain-openai<0.2.0,>=0.1.7, but you have langchain-openai 0.1.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-openai-0.1.6\nDefaulting to user installation because normal site-packages is not writeable\nCollecting langchain-pinecone==0.1.0\n  Downloading langchain_pinecone-0.1.0-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain-pinecone==0.1.0) (0.1.53)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-pinecone==0.1.0) (1.26.4)\nCollecting pinecone-client<4.0.0,>=3.2.2 (from langchain-pinecone==0.1.0)\n  Downloading pinecone_client-3.2.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (6.0.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (0.1.147)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (23.2)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (2.7.1)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (8.5.0)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone==0.1.0) (2025.1.31)\nRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone==0.1.0) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone==0.1.0) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone==0.1.0) (2.3.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (0.27.2)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (3.10.15)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (2.32.3)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (1.0.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (2.18.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (4.8.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (3.10)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (3.4.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone==0.1.0) (1.2.2)\nDownloading langchain_pinecone-0.1.0-py3-none-any.whl (8.4 kB)\nDownloading pinecone_client-3.2.2-py3-none-any.whl (215 kB)\nInstalling collected packages: pinecone-client, langchain-pinecone\n  Attempting uninstall: pinecone-client\n    Found existing installation: pinecone-client 4.0.0\n    Uninstalling pinecone-client-4.0.0:\n      Successfully uninstalled pinecone-client-4.0.0\nSuccessfully installed langchain-pinecone-0.1.0 pinecone-client-3.2.2\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: tiktoken==0.7.0 in /usr/local/lib/python3.10/dist-packages (0.7.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.7.0) (2023.12.25)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.7.0) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2025.1.31)\nDefaulting to user installation because normal site-packages is not writeable\nCollecting datasets==2.19.1\n  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (18.1.0)\nCollecting pyarrow-hotfix (from datasets==2.19.1)\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (0.70.16)\nCollecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.1)\n  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (0.28.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.1) (6.0.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.1) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets==2.19.1) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.19.1) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.19.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.19.1) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.19.1) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.1) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.1) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.1) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.1) (1.16.0)\nDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\nDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, fsspec, datasets\n\u001b[33m  WARNING: The script datasets-cli is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0mSuccessfully installed datasets-2.19.1 fsspec-2024.3.1 pyarrow-hotfix-0.6\nDefaulting to user installation because normal site-packages is not writeable\nCollecting typing_extensions==4.11.0\n  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\nDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\nInstalling collected packages: typing_extensions\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nembedchain 0.1.113 requires langchain-openai<0.2.0,>=0.1.7, but you have langchain-openai 0.1.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed typing_extensions-4.11.0\n"}]},{"source":"## Task 1: Building a Chatbot","metadata":{},"id":"92a9caca-70fd-4ac0-aa15-1bee55c456d3","cell_type":"markdown"},{"source":"We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To get more familiar with the library let's first create a chatbot _without_ RAG.\n","metadata":{},"id":"b1fcd794-b29c-4010-8be0-651a452b2044","cell_type":"markdown"},{"source":"### Instructions\n\nInitialize the chat model object.\n\n- *Make sure you have defined the `OPENAI_API_KEY` environment variable and connected it. See the 'Setting up DataLab Integrations' section of getting-started.ipynb.*\n- From the `langchain_openai` package, import `ChatOpenAI`.\n- Initialize a `ChatOpenAI` object with the `gpt-3.5-turbo` model. Assign to `chat`.","metadata":{},"id":"4cc8e24a-bd51-483a-81a6-e0c2d1e02c35","cell_type":"markdown"},{"source":"### How are chats structured?\n\nChats with OpenAI's `gpt-3.5-turbo` and `gpt-4` chat models are typically structured (in plain text) like this:\n\n```\nSystem: You are a helpful assistant.\n\nUser: Hi AI, how are you today?\n\nAssistant: I'm great thank you. How can I help you?\n\nUser: I'd like to understand string theory.\n\nAssistant:\n```\n\nThe final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n\n```python\n[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n]\n```\n\nIn LangChain there is a slightly different format. We use three _message_ objects like so:\n\n```python\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hi AI, how are you today?\"),\n    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n    HumanMessage(content=\"I'd like to understand string theory.\")\n]\n```\n\nThe format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.","metadata":{},"id":"ab269915-cec5-4015-a848-25ac67ac2b5a","cell_type":"markdown"},{"source":"### Instructions\n\nCreate a conversation.\n\n- From langchain's schema module, import the three message types: `SystemMessage`, `HumanMessage`, and `AIMessage`.\n- Create a conversation as a list of messages. Assign to `messages`.\n    1. A system message with content `\"You are a helpful assistant.\"`\n    2. A human message with content `\"Hi AI, how are you today?\"`\n    3. An AI message with content `\"I'm great thank you. How can I help you?\"`\n    4. A human message with content `\"I'd like to understand string theory.\"`\n","metadata":{},"id":"889de625-b7f6-493f-81a7-25e30bacb98b","cell_type":"markdown"},{"source":"# From the langchain.schema module, import SystemMessage, HumanMessage, AIMessage\nfrom langchain.schema import SystemMessage, HumanMessage, AIMessage\n\n# Create a conversation as a list of messages. Assign to messages.\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hi AI, how are you today?\"),\n    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n    HumanMessage(content=\"I'd like to understand string theory.\")\n]","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1741916508490,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# From the langchain.schema module, import SystemMessage, HumanMessage, AIMessage\nfrom langchain.schema import SystemMessage, HumanMessage, AIMessage\n\n# Create a conversation as a list of messages. Assign to messages.\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hi AI, how are you today?\"),\n    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n    HumanMessage(content=\"I'd like to understand string theory.\")\n]","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"8f542ae9-c4a0-41aa-a6b6-45585990c246","cell_type":"code","execution_count":8,"outputs":[]},{"source":"We generate the next response from the AI by passing these messages to the `ChatOpenAI` object. You can call `chat` as though it is a function.","metadata":{},"id":"7f6c51c7-bbbb-4f0f-b218-850221f3dcdf","cell_type":"markdown"},{"source":"### Instructions\n\nChat with GPT.\n\n- Invoke a chat with GPT, passing the messages, and get a response. Assign to `res`.\n- Print the response.","metadata":{},"id":"2a974f3e-c9ae-436c-880b-7634c334a786","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n    \nInvoke a chat with the `.invoke()` method of the `ChatOpenAI()` object, passing the list of messages.\n\n</p>\n</details>","metadata":{},"id":"0bafbb20-cdce-4709-aa7a-84c704fc42fa","cell_type":"markdown"},{"source":"# First, ensure you have the necessary Azure SDK packages installed\n# You can install them using pip in your Jupyter Notebook\n!pip install azure-ai-inference azure-core\n\nimport os\nfrom azure.ai.inference import ChatCompletionsClient\nfrom azure.ai.inference.models import SystemMessage, UserMessage\nfrom azure.core.credentials import AzureKeyCredential\n\n# Set the environment variable dynamically (instead of defining it in DataCamp Workspace settings)\nos.environ[\"OPENAI_API_KEY\"] = \"<API_KEY>\"\n\n# Retrieve the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Azure OpenAI endpoint and model\nendpoint = \"<Azure endpoint>\"\nmodel_name = \"gpt-3.5-turbo\"\n\n# Initialize the client using the API key from the environment variable\nclient = ChatCompletionsClient(\n    endpoint=endpoint,\n    credential=AzureKeyCredential(api_key),\n)\n\nprint(\"Azure OpenAI client initialized successfully.\")\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":395,"type":"stream"}}},"cell_type":"code","id":"d8a849c1-cc6b-45e4-8c35-c077233d9164","outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting azure-ai-inference\n  Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl.metadata (34 kB)\nCollecting azure-core\n  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from azure-ai-inference) (0.6.1)\nRequirement already satisfied: typing-extensions>=4.6.0 in /home/repl/.local/lib/python3.10/site-packages (from azure-ai-inference) (4.11.0)\nRequirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core) (2.32.3)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core) (2025.1.31)\nDownloading azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\nDownloading azure_core-1.32.0-py3-none-any.whl (198 kB)\nInstalling collected packages: azure-core, azure-ai-inference\nSuccessfully installed azure-ai-inference-1.0.0b9 azure-core-1.32.0\nAzure OpenAI client initialized successfully.\n"}],"execution_count":9},{"source":"from langchain.chat_models import AzureChatOpenAI\n\n# Retrieve API key and endpoint from environment variables\napi_key = os.getenv(\"OPENAI_API_KEY\")\nendpoint = \"<azure-endpoint>\"\ndeployment_name = \"gpt-3.5-turbo\"  # Replace with your actual deployment name\n\n# Ensure the API key exists\nif not api_key:\n    raise ValueError(\"Azure OpenAI API key is missing.\")\n\n# Initialize the Azure OpenAI model for LangChain\nchat = AzureChatOpenAI(\n    azure_endpoint=endpoint,\n    deployment_name=deployment_name,\n    api_key=api_key,\n    api_version=\"2023-05-15\",  # Use the correct API version from Azure\n)\n\nprint(\"Azure OpenAI model is initialized.\")\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":143,"type":"stream"}}},"cell_type":"code","id":"15e11d12-cf8c-4d5b-a1e8-e75ca5dda94f","outputs":[{"output_type":"stream","name":"stdout","text":"Azure OpenAI model is initialized.\n"},{"output_type":"stream","name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning:\n\nThe class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n\n"}],"execution_count":10},{"source":"# Invoke a chat with GPT, passing the messages\nres = chat.invoke(messages)\n\n# Print the response\nres","metadata":{"executionCancelledAt":null,"executionTime":1813,"lastExecutedAt":1741916576428,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Invoke a chat with GPT, passing the messages\nres = chat.invoke(messages)\n\n# Print the response\nres","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"47ad31bf-db2a-444b-a011-26e9336a4e1e","cell_type":"code","execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":"AIMessage(content=\"String theory is a theoretical framework in physics that attempts to reconcile general relativity and quantum mechanics. It posits that the fundamental building blocks of the universe are not particles but rather tiny, vibrating strings. These strings can exist in multiple dimensions, and their vibrations determine the properties of particles we observe in the universe.\\n\\nString theory suggests that there are multiple dimensions beyond the familiar three spatial dimensions and one time dimension. This idea can potentially explain phenomena that are beyond the reach of current theories, such as the existence of dark matter and dark energy.\\n\\nHowever, it's important to note that string theory is still a highly speculative and complex area of physics, and it has not yet been experimentally verified. Researchers continue to work on developing the theory further and exploring its implications for our understanding of the universe.\", response_metadata={'token_usage': {'completion_tokens': 158, 'prompt_tokens': 53, 'total_tokens': 211}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'stop', 'logprobs': None}, id='run-f25c129c-21c4-4467-964e-212e0c0441fd-0')"},"metadata":{},"execution_count":11}]},{"source":"Notice that the `AIMessage` object looks a bit like a dictionary. The most important element is `content`, which contains the chat text.","metadata":{},"id":"20487069-0c9e-4480-ba09-a6004686b3ba","cell_type":"markdown"},{"source":"### Instructions\n\nPrint only the contents of the response.","metadata":{},"id":"dd3463e9-26e8-4908-ab2c-aa78a147e79f","cell_type":"markdown"},{"source":"# Print the contents of the response\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":206,"type":"stream"}},"lastExecutedByKernel":null},"id":"38e8a99c-58ea-43ac-8918-64d42541a58d","cell_type":"code","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"String theory is a theoretical framework in physics that attempts to reconcile general relativity and quantum mechanics. It posits that the fundamental building blocks of the universe are not particles but rather tiny, vibrating strings. These strings can exist in multiple dimensions, and their vibrations determine the properties of particles we observe in the universe.\n\nString theory suggests that there are multiple dimensions beyond the familiar three spatial dimensions and one time dimension. This idea can potentially explain phenomena that are beyond the reach of current theories, such as the existence of dark matter and dark energy.\n\nHowever, it's important to note that string theory is still a highly speculative and complex area of physics, and it has not yet been experimentally verified. Researchers continue to work on developing the theory further and exploring its implications for our understanding of the universe.\n"}]},{"source":"Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation.","metadata":{},"id":"7090e9e9-880d-4b29-909a-a9653474c8b1","cell_type":"markdown"},{"source":"### Instructions\n\nContinue the conversation with GPT.\n\n- Append the latest AI response to `messages`.\n- Create a new human message. Assign to `prompt`.\n    - Use the content `\"Why do physicists believe it can produce a 'unified theory'?\"`\n- Append the prompt to messages.","metadata":{},"id":"b25f1b56-22b0-414c-bc56-99ea90820f1a","cell_type":"markdown"},{"source":"# Append the latest AI response to messages\nmessages.append(res)","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1741916591791,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Append the latest AI response to messages\nmessages.append(res)","outputsMetadata":{"0":{"height":505,"type":"stream"}},"lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"51e66e15-dc5d-4f5e-9db2-680f4c90546f","cell_type":"code","execution_count":13,"outputs":[]},{"source":"# Create a new human message. Assign to prompt.\nprompt = HumanMessage(content = \"Why do physicists believe it can produce a 'unified theory'?\")\n\n# Append the prompt to messages\nmessages.append(prompt)","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1741916595214,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a new human message. Assign to prompt.\nprompt = HumanMessage(content = \"Why do physicists believe it can produce a 'unified theory'?\")\n\n# Append the prompt to messages\nmessages.append(prompt)","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"be31574f-4801-452e-94a1-a544e50fb4b1","cell_type":"code","execution_count":14,"outputs":[]},{"source":"# Sanity check before you send to GPT: what does messages contain?\nres = chat(messages)\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"lastExecutedByKernel":null,"outputsMetadata":{"0":{"height":101,"type":"stream"},"1":{"height":290,"type":"stream"}}},"id":"7bc0c721-8eb2-4a20-8491-89a281480ea5","cell_type":"code","execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning:\n\nThe method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n\n"},{"output_type":"stream","name":"stdout","text":"Physicists believe that string theory has the potential to produce a unified theory because it aims to describe all fundamental forces and particles in a single framework. In traditional physics, there are separate theories to describe different forces, such as general relativity for gravity and the Standard Model for the other three fundamental forces (electromagnetism, weak nuclear force, and strong nuclear force).\n\nOne of the main goals of a unified theory, often referred to as the \"Theory of Everything,\" is to provide a single framework that can explain all known phenomena in the universe. By positing that all particles are made up of vibrating strings, string theory attempts to unify gravity with the other fundamental forces in a consistent manner.\n\nIf successful, a unified theory like string theory could provide a deeper understanding of the fundamental nature of reality and potentially resolve some of the long-standing puzzles in physics, such as the incompatibility between general relativity and quantum mechanics.\n\nHowever, it's important to emphasize that string theory is still a work in progress, and it has not yet been experimentally confirmed. Researchers continue to investigate its implications and strive to develop a more complete and testable version of the theory.\n"}]},{"source":"### Instructions\n\n- Invoke the chat again to send the messages to GPT. Assign to `res`.\n- Print the contents of the response.","metadata":{},"id":"e3628e65-9dde-41b2-bf22-e489fb70694a","cell_type":"markdown"},{"source":"# Invoke the chat again to send the messages to GPT. Assign to res.\nres = chat.invoke(messages)\n\n# Print the contents of the response\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":311,"type":"stream"},"1":{"height":353,"type":"stream"}},"lastExecutedByKernel":null},"id":"4f68e1d4-4dcc-4dbe-b7ce-df321f90ae5c","cell_type":"code","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":"Physicists believe that string theory has the potential to produce a unified theory because it aims to describe all fundamental forces and particles in a single framework. Currently, there are two main pillars of modern physics: general relativity, which describes gravity on a large scale, and quantum mechanics, which describes the behavior of particles on a small scale.\n\nOne of the main challenges in physics is to reconcile these two theories into a single, coherent framework known as a \"unified theory\" or \"theory of everything.\" String theory offers a promising approach to achieving this goal by providing a theoretical framework that can potentially unify all fundamental forces and particles into a single, consistent description.\n\nBy treating particles as vibrating strings instead of point-like objects, string theory naturally incorporates both quantum mechanics and general relativity. This suggests that it could be the key to resolving the inconsistencies between these two theories and providing a more complete understanding of the fundamental workings of the universe.\n\nWhile string theory has not yet been experimentally confirmed, many physicists are optimistic about its potential to provide a unified theory that can explain the fundamental forces of nature and the behavior of particles in a consistent and comprehensive manner.\n"}]},{"source":"## Task 2: Hallucinations","metadata":{},"id":"c536df54-a985-401e-99e7-212004379618","cell_type":"markdown"},{"source":"We have our chatbot, but as mentioned—the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n\n\n\nBy default, LLMs have no access to the external world.\n\n![langchain-no-access-to-world](langchain-no-access-to-world.png)\n\nThis means that GPT (or any other LLM) will perform badly on some types of question.\n\n* The chatbot doesn't know about recent events. How does it respond if you ask about the weather in your city today?\n* It can't answer questions about recent code or recent products. Try asking it `\"Can you tell me about the LLMChain in LangChain?\"` or `\"What was the latest course released on DataCamp?\"`\n* It can't answer questions about confidential corporate information that hasn't been released into the internet.","metadata":{},"id":"e257b696-4c54-4412-9534-7bf57a47cfc3","cell_type":"markdown"},{"source":"### Instructions\n\nAppend the AI response to the list of messages.\n\n- Print the number of messages in the conversation.\n- Append the latest AI response to `messages`.\n- Print the number of messages in the conversation again.","metadata":{},"id":"61ae0a3c-6d1a-4616-9cf0-871c1dd4b5d5","cell_type":"markdown"},{"source":"# Print the number of messages in the conversation\nprint(len(messages))\n\n# Append the response to the list of messages\nmessages.append(res)\n\n# Print the number of messages in the conversation again\nprint(len(messages))","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1741916629749,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Print the number of messages in the conversation\nprint(len(messages))\n\n# Append the response to the list of messages\nmessages.append(res)\n\n# Print the number of messages in the conversation again\nprint(len(messages))","outputsMetadata":{"0":{"height":59,"type":"stream"}},"lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"5af4c597-b1b6-4159-9a86-876789695fab","cell_type":"code","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":"6\n7\n"}]},{"source":"### Instructions\n\nAsk GPT about Llama 3.\n\n- Create a new human message. Assign to `prompt`.\n    - Use the content `\"What is so special about Llama 3?\"`.\n- Append the prompt to `messages`.\n- Invoke the chat to send the messages to GPT. Assign to `res`.\n- Print the contents of the response.","metadata":{},"id":"985b4acd-0c16-457c-837f-50a2a3f2b9d1","cell_type":"markdown"},{"source":"# Create a new human message about Llama 3\nprompt = HumanMessage(content = \"What is so special about Llama 3?\")\n\n# Append this message to the conversation. Assign to prompt.\nmessages.append(prompt)\n\n# Invoke the chat with the latest list of messages\nres =chat.invoke(messages)\n\n# Print the contents of the response\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":38,"type":"stream"}},"lastExecutedByKernel":null},"id":"c3e22ec5-7a01-46a2-96b9-ee2dda555932","cell_type":"code","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":"I'm not certain what you are referring to with \"Llama 3.\" Could you provide more context or clarify your question so I can better assist you?\n"}]},{"source":"### Confidently wrong: hallucinations from LLMs\n\nOur chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the information, but sometimes an LLM may respond like it _does_ know the answer—and this can be very hard to detect. See this example from the earliest version of GPT-4 in the OpenAI Playground:\n\n![llm-chain-hallucination](llm-chain-hallucination.png)\n\nOpenAI have since adjusted the behavior for this particular example as we can see below:\n","metadata":{},"id":"632d8027-3fe6-4e5d-ba5e-bbd261c6e9d6","cell_type":"markdown"},{"source":"### Instructions\n\nAsk GPT about LangChain.\n\n- Append the latest AI response to `messages`.\n- Create a new human message. Assign to `prompt`.\n    - Use the content `\"Can you tell me about the LLMChain in LangChain?\"`.\n- Append the prompt to `messages`.\n- Send the messages to GPT. Assign to `res`.\n- Print the contents of the response.","metadata":{},"id":"2fdf6188-f9ad-42fd-b254-cfed65434024","cell_type":"markdown"},{"source":"# Append the latest AI response to messages\nmessages.append(res)\n\n# Create a new human message. Assign to prompt.\nprompt = HumanMessage(content = \"Can you tell me about the LLMChain in LangChain?\")\n\n# Append the latest prompt to messages\nmessages.append(prompt)\n\n# Invoke the chat with the latest list of message\nres = chat.invoke(messages)\n\n# Print the contents of the response\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":80,"type":"stream"}},"lastExecutedByKernel":null},"id":"13e182b4-3b48-4d28-88dd-27abc9ad4a1c","cell_type":"code","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"I apologize, but I am not familiar with the specific terms \"LLMChain\" and \"LangChain.\" It is possible that they refer to specialized concepts, technologies, or projects that are not widely known. If you can provide more context or details about LLMChain in LangChain, I may be able to offer more assistance.\n"}]},{"source":"There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation.","metadata":{},"id":"ef46c06c-d226-49ce-b467-78408945ff4f","cell_type":"markdown"},{"source":"### Instructions\n\nCreate a string of knowledge about chains.\n\n- *Read the descriptions of LLMChains, Chains, and LangChain given in `llmchain_information`.*\n- Combine the list of description strings into a single string. Assign to `source_knowledge`.","metadata":{},"id":"9e8fb83d-8c94-4d11-b21a-12c455a7f873","cell_type":"markdown"},{"source":"# Run these descriptions of LLMChains, Chains, and LangChain \nllmchain_information = [\n    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n]\nlen(llmchain_information)","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1741916669623,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Run these descriptions of LLMChains, Chains, and LangChain \nllmchain_information = [\n    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n]\nlen(llmchain_information)","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"711778a5-76d5-40d2-8ed1-2b12a89a474d","cell_type":"code","execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{},"execution_count":20}]},{"source":"# Run this to join the definitions, separated by newlines\nsource_knowledge = \"\\n\".join(llmchain_information)\nsource_knowledge","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1741916677235,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Run this to join the definitions, separated by newlines\nsource_knowledge = \"\\n\".join(llmchain_information)\nsource_knowledge","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"f63762f2-0cb2-48ee-8d51-8fc6b3165fcd","cell_type":"code","execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\\nChains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.'"},"metadata":{},"execution_count":21}]},{"source":"We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query.","metadata":{},"id":"961fe8f0-4b6a-4227-8b40-0ab501c13ee5","cell_type":"markdown"},{"source":"### Instructions\n\n- Define a question. Assign to `query`.\n    - Use the text `\"Can you tell me about the LLMChain in LangChain?\"`\n- Create an augmented prompt containing the context and query. Assign to `augmented_prompt`.\n\n        augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\n        Contexts:\n        {source_knowledge}\n\n        Query: {query}\"\"\"","metadata":{},"id":"a18cd1f5-ace4-4abd-b139-6d6e163e9c3d","cell_type":"markdown"},{"source":"# Define a question. Assign to query\nquery = \"Can you tell me about the LLMChain in LangChain?\"\n\n# Create an augmented prompt containing the context and query. Assign to augmented_prompt\naugmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\n  Contexts:\n  {source_knowledge}\n\n  Query: {query}\"\"\"","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1741916685864,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define a question. Assign to query\nquery = \"Can you tell me about the LLMChain in LangChain?\"\n\n# Create an augmented prompt containing the context and query. Assign to augmented_prompt\naugmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\n  Contexts:\n  {source_knowledge}\n\n  Query: {query}\"\"\"","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"58a71fbf-6b81-42f5-8073-ddfd3139e351","cell_type":"code","execution_count":22,"outputs":[]},{"source":"# Print the augmented prompt\nprint(augmented_prompt)","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1741916691963,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Print the augmented prompt\nprint(augmented_prompt)","outputsMetadata":{"0":{"height":332,"type":"stream"}},"lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"99e5faa0-e6dd-43a1-ba33-96fdeea31f9e","cell_type":"code","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":"Using the contexts below, answer the query. If some information is not provided within the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\n  Contexts:\n  A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\nChains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n\n  Query: Can you tell me about the LLMChain in LangChain?\n"}]},{"source":"Now we feed this into our chatbot as we did before.\n\nDon't append the previous AI message, since it wasn't a good answer.","metadata":{},"id":"686b36f9-9107-4099-b0da-22e60e4b17e6","cell_type":"markdown"},{"source":"### Instructions\n\nInclude the augmented prompt in the conversation.\n\n- Print the last message in the list.\n- Replace the last message with a human message containing the augmented prompt.","metadata":{},"id":"aa19e06d-a9c0-4b25-bf3b-072b2f163852","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n    \nThe last element of a list can be accessed with the position `-1`.\n    \n```py\n# Get the last element of a list\nlst[-1]\n\n# Replace the last element of a list\nlst[-1] = new_value\n```\n\n</p>\n</details>","metadata":{},"id":"55e6f4b0-3d2a-445d-82d6-9dfc8635db50","cell_type":"markdown"},{"source":"# Print the last message in the conversation\nmessages[-1]","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1741916700707,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Print the last message in the conversation\nmessages[-1]","outputsMetadata":{"0":{"height":38,"type":"stream"}},"lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"a7e0ee47-69f2-4f21-b48d-279e9ae5df3e","cell_type":"code","execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":"HumanMessage(content='Can you tell me about the LLMChain in LangChain?')"},"metadata":{},"execution_count":24}]},{"source":"# Replace the last message with a human message containing the augmented prompt\nmessages[-1] = HumanMessage(content = augmented_prompt)","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1741916706831,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Replace the last message with a human message containing the augmented prompt\nmessages[-1] = HumanMessage(content = augmented_prompt)","outputsMetadata":{"0":{"height":368,"type":"stream"}},"lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"901893e6-726d-4a42-805f-ee2d24b3b8a2","cell_type":"code","execution_count":25,"outputs":[]},{"source":"### Instructions\n\nAsk GPT about LangChain again, this time providing source knowledge.\n\n- Send the messages to GPT. Assign to `res`.\n- Print the contents of the response.","metadata":{},"id":"5f95104d-51aa-4c36-93fb-684e4ea99f8d","cell_type":"markdown"},{"source":"# Invoke the chat with the list of messages\nres = chat.invoke(messages)\n\n# Print the contents of the response\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"lastExecutedByKernel":null,"outputsMetadata":{"0":{"height":101,"type":"stream"}}},"id":"ebbf365d-0215-4f6f-bb5b-146323b559b4","cell_type":"code","execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":"LLMChain in LangChain is the most common type of chain that consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, formats them into a prompt using the PromptTemplate, passes that to the model, and then uses the OutputParser (if provided) to parse the output of the LLM into a final format. It is designed to enable applications powered by language models to be data-aware and agentic, connecting language models to other data sources and allowing them to interact with their environment.\n"}]},{"source":"The quality of this answer is phenomenal! This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem—how do we get this information in the first place?\n\nWe learned in the previous code-alongs about Pinecone and vector databases. Well, they can help us here too. But first, we'll need a dataset.","metadata":{},"id":"30ae1f20-1e41-4e4f-a08e-09d0167170ba","cell_type":"markdown"},{"source":"## Task 3: Importing the Data","metadata":{},"id":"97fa3cfe-9223-4593-85fd-6c21f788d46e","cell_type":"markdown"},{"source":"In this task, we will be importing our data. We will be using the Hugging Face Datasets library and [the `\"jamescalam/llama-2-arxiv-papers\"` dataset](https://huggingface.co/datasets/jamescalam/llama-2-arxiv-papers-chunked). This dataset contains a collection of ArXiv papers which will serve as the external knowledge base for our chatbot.","metadata":{},"id":"d6f4cd0f-4d29-4667-bf90-2603bae73759","cell_type":"markdown"},{"source":"### Instructions\n\nLoad the ArXiv papers dataset.\n\n- From the *datasets* package, import `load_dataset`.\n- Load the train split of the `jamescalam/llama-2-arxiv-papers-chunked` dataset. Assign to `dataset`.\n- Print the dataset object to see the structure of the data.\n- *Look at the structure. Which fields should we keep?*","metadata":{},"id":"404b9a07-65b0-490b-9380-fda08573baad","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n    \nTo load the training part of a Hugging Face dataset, call `load_dataset()`, passing the dataset name, and setting `split` to `\"train\"`.\n\n</p>\n</details>","metadata":{},"id":"87ded94b-84f9-4fc2-9f64-2607ee4f8c8b","cell_type":"markdown"},{"source":"# From the datasets package, import load_dataset\nfrom datasets import load_dataset\n\n# Load the arxiv dataset, training set only\ndata = load_dataset(\"jamescalam/llama-2-arxiv-papers-chunked\", split= \"train\")\n\n# Print the dataset object\ndata","metadata":{"executionCancelledAt":null,"executionTime":2751,"lastExecutedAt":1741916726564,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# From the datasets package, import load_dataset\nfrom datasets import load_dataset\n\n# Load the arxiv dataset, training set only\ndata = load_dataset(\"jamescalam/llama-2-arxiv-papers-chunked\", split= \"train\")\n\n# Print the dataset object\ndata","outputsMetadata":{"1":{"height":76,"type":"stream"},"6":{"height":95,"type":"stream"}},"lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"16f97002-b358-4a33-bb50-0401feda259b","cell_type":"code","execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/409 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"656c273da06141f49d68a5de8f6aad5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/14.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc0fad2e4df74d2b86d5f12f405558c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/4838 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21968fa67fb94061afcbf35ffd38c275"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n    num_rows: 4838\n})"},"metadata":{},"execution_count":27}]},{"source":"### Instructions\n\nPrint a record of dataset to get a feel for what they contain.","metadata":{},"id":"796c12aa-e78a-4e62-9205-73cc6f3908cd","cell_type":"markdown"},{"source":"# Print a record of dataset\ndata[0]","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1741916733389,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Print a record of dataset\ndata[0]","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"0e4c8dfd-2c65-4d2f-8ae1-d356daf0a3d8","cell_type":"code","execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":"{'doi': '1102.0183',\n 'chunk-id': '0',\n 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but',\n 'id': '1102.0183',\n 'title': 'High-Performance Neural Networks for Visual Object Classification',\n 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n 'source': 'http://arxiv.org/pdf/1102.0183',\n 'authors': ['Dan C. Cireşan',\n  'Ueli Meier',\n  'Jonathan Masci',\n  'Luca M. Gambardella',\n  'Jürgen Schmidhuber'],\n 'categories': ['cs.AI', 'cs.NE'],\n 'comment': '12 pages, 2 figures, 5 tables',\n 'journal_ref': None,\n 'primary_category': 'cs.AI',\n 'published': '20110201',\n 'updated': '20110201',\n 'references': []}"},"metadata":{},"execution_count":28}]},{"source":"### Dataset Summary\n\nThe dataset we are using is sourced from the Llama 2 ArXiv papers. It is a collection of academic papers from ArXiv, a repository of electronic preprints approved for publication after moderation. Each entry in the dataset represents a \"chunk\" of text from these papers.\n\nBecause most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, they cannot answer our questions about Llama 2—at least not without this data.","metadata":{},"id":"7901bcab-0c5e-4bc1-944e-ebd13d9284da","cell_type":"markdown"},{"source":"## Task 4: Building the Knowledge Base","metadata":{},"id":"7af0beaa-68f8-4cc3-831f-1118e79f45b3","cell_type":"markdown"},{"source":"We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.","metadata":{},"id":"92558679-3d66-419d-b30d-783456b992e5","cell_type":"markdown"},{"source":"### Workflow\n\nThe workflow for setting up a chatbot is much the same as for setting up semantic serach and retrieval augmented generation.\n\n- Initialize your connection to the Pinecone vector DB.\n- Create an index (remember to consider the dimensionality of `text-embedding-ada-002`).\n- Initialize OpenAI's `text-embedding-ada-002` model with LangChain.\n- Populate the index with records (in this case from the Llama 2 dataset).","metadata":{},"id":"2f5e123a-23f5-4fbb-a8dd-226dbdbd5b7f","cell_type":"markdown"},{"source":"### Instructions\n\nInitialize Pinecone, getting setup details from Workspace environment variables.\n\n- Import the os package.\n- Import the pinecone package.\n- Initialize pinecone, setting the API key. Assign to `pc`.","metadata":{},"id":"b1b2ef0b-ce4b-48ae-a3cb-8fba1143f6af","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n    \nThe Pinecone environment variable is usually called `PINECONE_API_KEY`, but check what you called it!\n    \n---\n    \nTo initialize Pinecone, call `pinecone.Pinecone()`, setting `api_key` to the API key.\n\n</p>\n</details>","metadata":{},"id":"00c1f717-3534-4c29-b7fe-44bb9002e791","cell_type":"markdown"},{"source":"# Import the os and pinecone packages\n!pip install pinecone\nimport os\nimport pinecone\n\n# Create a Pinecone object. Assign to pc.\npc = pinecone.Pinecone(api_key = \"<PINECONE_API_KEY>\")","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"lastExecutedByKernel":null,"outputsMetadata":{"0":{"height":269,"type":"stream"}}},"id":"0be5dcfe-1fd1-432c-9c95-77572f27e2fe","cell_type":"code","execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting pinecone\n  Downloading pinecone-6.0.2-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2025.1.31)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /home/repl/.local/lib/python3.10/site-packages (from pinecone) (0.0.7)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.9.0.post0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /home/repl/.local/lib/python3.10/site-packages (from pinecone) (4.11.0)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.3.0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\nDownloading pinecone-6.0.2-py3-none-any.whl (421 kB)\nInstalling collected packages: pinecone\nSuccessfully installed pinecone-6.0.2\n"}]},{"source":"Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`.","metadata":{},"id":"6536757a-943a-4b06-bba2-6bfe4152d170","cell_type":"markdown"},{"source":"### Instructions\n\nCreate a vector index in the Pinecone database.\n\n- Import the time package.\n- Choose a name for the vector index. Assign to `index_name`.\n- Check if index_name is not in Pinecone's list of existing indexes.\n    -  Create an index named index_name, dimension 1536, cosine similarity as its metric.\n    -  While the index status is not ready, sleep for one second.","metadata":{},"id":"e1b833dd-0ed2-4547-b0c4-18e9a105963f","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n    \nGet the list of available indexes with `pc.list_indexes()`. The code pattern to get all available index names is as follows.\n    \n```py\n[idx.name for idx in pc.list_indexes().indexes]\n```\n    \n---\n    \nCreate an index with `pc.create_index()`, passing the index name, and setting the dimension, metric, and spec. In theory, you can specify where in the cloud Pinecone runs. Currently, Pinecone Serverless only runs on AWS in limited locations. Try `us-east-1` first and `us-west-2` as a backup. The code pattern to create an index is as follows.\n    \n```py\npc.create_index(\n        index_name,\n        dimension=n_dims,\n        metric=\"cosine|dotproduct|euclidean\",\n        spec=pinecone.ServerlessSpec(\n            cloud=\"aws\",\n            region=\"us-east-1\"\n        )\n    )\n```\n    \n---\n    \nGet the index details with `pc.describe_index(index_name)`. The code pattern to check that the index is ready is as follows.\n    \n```py\npc.describe_index(index_name).status[\"ready\"]\n```\n\n---\n    \nThe code pattern for sleeping until a condition is met is as follows.\n    \n```py\nwhile not condition\n    time.sleep(n)\n```\n    \n</p>\n</details>","metadata":{},"id":"f205ffcf-0fcb-4e1e-8408-f100a92e4218","cell_type":"markdown"},{"source":"# Import the time package\nimport time\n\n# Define the index name\nindex_name = \"llama-3-rag\"\n\n# List the names of available indexes. Assign to existing_index_names.\nexisting_index_names = [idx.name for idx in pc.list_indexes().indexes]\n\n# Check if index_name is not in the list of available indexes\nif index_name not in pc.list_indexes():\n    # Create the index with index_name, a dimension of 1536, and the metric \"cosine\"\n    pc.create_index(\n        index_name,\n        dimension=1536,\n        metric=\"cosine\",\n        spec=pinecone.ServerlessSpec(\n            cloud=\"aws\",\n            region=\"us-east-1\"\n        )\n    )\n    # If the index status is not ready, sleep for 2 seconds\n    while not pc.describe_index(index_name).status[\"ready\"]:\n        time.sleep(2)","metadata":{"executionCancelledAt":null,"executionTime":6922,"lastExecutedAt":1741916797958,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the time package\nimport time\n\n# Define the index name\nindex_name = \"llama-3-rag\"\n\n# List the names of available indexes. Assign to existing_index_names.\nexisting_index_names = [idx.name for idx in pc.list_indexes().indexes]\n\n# Check if index_name is not in the list of available indexes\nif index_name not in pc.list_indexes():\n    # Create the index with index_name, a dimension of 1536, and the metric \"cosine\"\n    pc.create_index(\n        index_name,\n        dimension=1536,\n        metric=\"cosine\",\n        spec=pinecone.ServerlessSpec(\n            cloud=\"aws\",\n            region=\"us-east-1\"\n        )\n    )\n    # If the index status is not ready, sleep for 2 seconds\n    while not pc.describe_index(index_name).status[\"ready\"]:\n        time.sleep(2)","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"a9c31353-0b6c-40e4-8b13-582b39004f55","cell_type":"code","execution_count":31,"outputs":[]},{"source":"### Instructions\n\n- Connect to the index using its name. Assign to `index`.\n- View the index stats.","metadata":{},"id":"44cb4e16-e943-43ee-9555-028c9da19754","cell_type":"markdown"},{"source":"# Connect to the index using its name. Assign to index.\nindex = pc.Index(index_name)\n\n# View the index stats\nindex.describe_index_stats()","metadata":{"executionCancelledAt":null,"executionTime":152,"lastExecutedAt":1741916802661,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Connect to the index using its name. Assign to index.\nindex = pc.Index(index_name)\n\n# View the index stats\nindex.describe_index_stats()","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"ea941ab8-99ab-4600-b035-7a892b0bd88d","cell_type":"code","execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":"{'dimension': 1536,\n 'index_fullness': 0.0,\n 'metric': 'cosine',\n 'namespaces': {},\n 'total_vector_count': 0,\n 'vector_type': 'dense'}"},"metadata":{},"execution_count":32}]},{"source":"Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-ada-002` model—we can access it via LangChain.","metadata":{},"id":"79733e18-cb8a-4b0a-9335-d0bdc90529ff","cell_type":"markdown"},{"source":"### Instructions\n\nCreate an embeddings model.\n\n- From the `langchain_openai` package, import `OpenAIEmbeddings`.\n- Create an embedings model object for `text-embedding-ada-002`. Assign to `embed_model`.","metadata":{},"id":"b97b2f2c-828e-4517-b160-188977e9bbb5","cell_type":"markdown"},{"source":"import os\nfrom langchain_openai import AzureOpenAIEmbeddings\nfrom azure.core.credentials import AzureKeyCredential\n\n# Set the Azure OpenAI API key and endpoint\nOPENAI_API_KEY = \"<OPENAI_API_KEY>\"  \napi_key = os.getenv(\"OPENAI_API_KEY\")\nendpoint = \"<endpoint>\"\n\n# Create an embeddings model object for text-embedding-ada-002\nembed_model = AzureOpenAIEmbeddings(\n    model=\"text-embedding-ada-002\",\n    azure_endpoint=endpoint,\n    api_key=api_key,\n    api_version=\"2023-05-15\",  # Use the correct API version from Azure\n)","metadata":{"executionCancelledAt":null,"executionTime":96,"lastExecutedAt":1741916810244,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import os\nfrom langchain_openai import AzureOpenAIEmbeddings\nfrom azure.core.credentials import AzureKeyCredential\n\n# Set the Azure OpenAI API key and endpoint\nOPENAI_API_KEY = \"9GolQfh8x0JUJarullV4thrrUaOzco3XillzYFXtxeAfmoaOAIlFJQQJ99BAACHrzpqXJ3w3AAABACOGZIMT\"  \napi_key = os.getenv(\"OPENAI_API_KEY\")\nendpoint = \"https://iu-dphdi-research.openai.azure.com/\"\n\n# Create an embeddings model object for text-embedding-ada-002\nembed_model = AzureOpenAIEmbeddings(\n    model=\"text-embedding-ada-002\",\n    azure_endpoint=endpoint,\n    api_key=api_key,\n    api_version=\"2023-05-15\",  # Use the correct API version from Azure\n)","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680","outputsMetadata":{"0":{"height":143,"type":"stream"}}},"id":"7fc53547-8e89-4700-9a69-95bca1bedd30","cell_type":"code","execution_count":33,"outputs":[]},{"source":"Using this model we can create embeddings like so:","metadata":{},"id":"a7c8de50-0543-4ae0-92c0-1da97803f841","cell_type":"markdown"},{"source":"# Run this to see an example of the embeddings code pattern\ntexts = [\n    \"this is a sentence\",\n    \"this is another sentence\"\n]\n\nres = embed_model.embed_documents(texts=texts)\nlen(res), len(res[0])","metadata":{"executionCancelledAt":null,"executionTime":941,"lastExecutedAt":1741916817969,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Run this to see an example of the embeddings code pattern\ntexts = [\n    \"this is a sentence\",\n    \"this is another sentence\"\n]\n\nres = embed_model.embed_documents(texts=texts)\nlen(res), len(res[0])","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"5b86b13a-591f-4b8e-b3cc-33c24c076d5b","cell_type":"code","execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":"(2, 1536)"},"metadata":{},"execution_count":34}]},{"source":"From this we get two (aligning to our two chunks of text) 1536-dimensional embeddings.\n\nWe're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches.","metadata":{},"id":"f7f7f8ee-beb7-4dd0-86fa-cddf0d130d7e","cell_type":"markdown"},{"source":"### Instructions\n\nPrepare the data for upserting to Pinecone.\n\n- From *tqdm*, import `tqdm` (a progress bar).\n- Select these columns: `doi`, `chunk-id`, `chunk`, `title`, `source`. Assign to `data_selected`.\n- Convert `data_selected` to a pandas DataFrame in batch sizes of `100`. Assign to `data_batched`.","metadata":{},"id":"f2ae6daf-ff93-4083-a4ac-259c09c6ad0b","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n    \nSelect columns from a Hugging Face dataset with the `.select_columns()` method, passing a list of column names.\n    \n```py\ndata.select_columns([\"column1\", \"column2\"])\n```\n\n---\n\nConvert a Hugging Face dataset to a pandas DataFrame with the `.to_pandas()` method. Set `batched` to `True` and `batch_size` to a positive integer to create a batched version of the dataset.\n    \n```py\ndata.to_pandas(batched=True, batch_size=n)\n```\n\nNote that this returns a generator for a DataFrame (rather than a DataFrame). That means you can't access the contents until you use it inside a loop.\n    \n</p>\n</details>","metadata":{},"id":"afdd4bf0-b806-46c7-b88c-dd4a18ac531e","cell_type":"markdown"},{"source":"# From the tqdm package, import tqdm\nfrom tqdm import tqdm\n\n# Select these columns: doi, chunk-id, chunk, title, source. Assign to data_selected.\ndata_selected = data.select_columns([\"doi\", \"chunk-id\", \"chunk\", \"title\", \"source\"])\n\n# Convert data_selected to a pandas DataFrame in batch sizes of 100. Assign to data_batched.\ndata_batched = data.to_pandas(batched = True, batch_size = 100)","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1741916825085,"lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# From the tqdm package, import tqdm\nfrom tqdm import tqdm\n\n# Select these columns: doi, chunk-id, chunk, title, source. Assign to data_selected.\ndata_selected = data.select_columns([\"doi\", \"chunk-id\", \"chunk\", \"title\", \"source\"])\n\n# Convert data_selected to a pandas DataFrame in batch sizes of 100. Assign to data_batched.\ndata_batched = data.to_pandas(batched = True, batch_size = 100)"},"id":"78901974-3c01-4c19-a916-6a53d7fb17ad","cell_type":"code","execution_count":35,"outputs":[]},{"source":"### Instructions\n\nSplit the dataset into batches and add it to the vector index.\n\n- Loop over each batch in `data_batched`, adding a progress bar.\n    - Concatenate the `doi` and `chunk-id` columns separated by `-`, then convert to a list. Assign to `ids`.\n    - Get the `chunk` column and convert to a list. Assign to `texts`.\n    - Use the embedding model to embed the texts. Assign to `embeds`.\n    - Get the metadata from the batch. Assign to metadata.\n        - Select the `chunk`, `title`, and `source` columns.\n        - Apply the `dict` function to the columns axis.\n        - Convert to a list.\n    - Combine IDs, embeddings, and metadata as list of tuples. Assign to `to_upsert`.\n    - Upsert to Pinecone.","metadata":{},"id":"4b0b83df-51bb-4ede-a388-76724fb98d0b","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n    \nThe code pattern to loop over the batched DataFrame generator is as follows.\n    \n```py\nfor batch in data_batched:\n    # batch is now a DataFrame\n    # do something with it\n```\n    \nThe variation of this with a progress bar is as follows.\n   \n```py\nfor batch in tqdm(data_batched):\n    # batch is now a DataFrame\n    # do something with it\n```\n    \n---\n    \nConcatenate text columns in a data with `+`.\n    \n```py\ndf[\"col1\"] + \"-\" + df[\"col2\"] \n```\n    \nConvert a pandas Series to a list with `.to_list()`. You'll need an extra pair of parentheses here.\n    \n```py\n(df[\"col1\"] + \"-\" + df[\"col2\"]).to_list()\n```   \n    \n---\n  \nEmbed documents with the `.embed_documents()` method of the embeddings model, passing the text as a list.\n    \n```py\nembed_model.embed_documents(list_of_documents)\n```\n    \n---\n\nPinecone wants the metadata as a list of dictionaries, not a DataFrame. \n    \n```\n[\n  {\"chunk\": \"chunk0\", \"title\": \"title0\", \"source\": \"source0\"},\n  {\"chunk\": \"chunk1\", \"title\": \"title1\", \"source\": \"source1\"},\n  {\"chunk\": \"chunk2\", \"title\": \"title2\", \"source\": \"source2\"},\n  ...\n]\n```\n\nThere are many ways of performing this conversion (though none of them are especially elegant). Use the method you are most comfortable with.\n    \n---\n\nCombine separate lists into a list of tuples using `zip()`.\n    \n```py\nzip(list0, list1, list2)\n```\n    \n---\n    \nUpsert to a Pinecone index using the index's `.upsert()` method, setting `vectors` to the tuples of ids, text, and metadata.\n    \n```py\nindex.upsert(vectors=to_upsert)\n```\n    \n</p>\n</details>","metadata":{},"id":"011dafe2-26b9-4ff5-9468-1644d9bfe76b","cell_type":"markdown"},{"source":"# Loop over each batch in data_batched, adding a progress bar\nfor batch in tqdm(data_batched):\n    # Concatenate the doi and chunk-id columns separated by -, then convert to a list. \n    # Assign to ids.\n    ids = (batch[\"doi\"] + \"-\" + batch[\"chunk-id\"]).to_list()\n    \n    # Get the chunk column and convert to a list. Assign to texts.\n    texts = batch[\"chunk\"].to_list()\n    \n    # Use the embeddings model to embed the texts. Assign to embeds.\n    embeds = embed_model.embed_documents(texts)\n    \n    # Get the metadata from the batch. Assign to metadata.\n    # Select the chunk, title, source columns\n    # Apply the dict function to the columns axis\n    # Convert to a list\n    metadata = batch \\\n        [[\"chunk\", \"title\", \"source\"]] \\\n        .apply(dict, axis=\"columns\") \\\n        .to_list()\n    \n    # Combine IDs, embeddings, and metadata as list of tuples. Assign to to_upsert.\n    to_upsert = zip(ids, embeds, metadata)\n    \n    # Upsert to Pinecone\n    index.upsert(vectors=to_upsert)","metadata":{"executionCancelledAt":null,"executionTime":616157,"lastExecutedAt":1741917449000,"lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Loop over each batch in data_batched, adding a progress bar\nfor batch in tqdm(data_batched):\n    # Concatenate the doi and chunk-id columns separated by -, then convert to a list. \n    # Assign to ids.\n    ids = (batch[\"doi\"] + \"-\" + batch[\"chunk-id\"]).to_list()\n    \n    # Get the chunk column and convert to a list. Assign to texts.\n    texts = batch[\"chunk\"].to_list()\n    \n    # Use the embeddings model to embed the texts. Assign to embeds.\n    embeds = embed_model.embed_documents(texts)\n    \n    # Get the metadata from the batch. Assign to metadata.\n    # Select the chunk, title, source columns\n    # Apply the dict function to the columns axis\n    # Convert to a list\n    metadata = batch \\\n        [[\"chunk\", \"title\", \"source\"]] \\\n        .apply(dict, axis=\"columns\") \\\n        .to_list()\n    \n    # Combine IDs, embeddings, and metadata as list of tuples. Assign to to_upsert.\n    to_upsert = zip(ids, embeds, metadata)\n    \n    # Upsert to Pinecone\n    index.upsert(vectors=to_upsert)","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"335b231c-6871-49ff-aacf-e7fc7d0121c5","cell_type":"code","execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":"49it [10:16, 12.57s/it]\n"}]},{"source":"We can check that the vector index has been populated using `describe_index_stats` like before:","metadata":{},"id":"58db36f2-1ba2-4fdb-9e72-aabe8fc16882","cell_type":"markdown"},{"source":"### Instructions\n\nCheck on updates to the vector index now that it contains the ArXiv dataset.\n\n- View the index stats again.\n- *What has changed since you last looked?*","metadata":{},"id":"2235eb44-2990-4d7e-ab51-6e36263d685f","cell_type":"markdown"},{"source":"# Get the index's descriptive statistics\nindex.describe_index_stats()","metadata":{"executionCancelledAt":null,"executionTime":82,"lastExecutedAt":1741919810392,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Get the index's descriptive statistics\nindex.describe_index_stats()","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"48aa5c57-c923-49fc-b1dd-d60f6fa26d64","cell_type":"code","execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":"{'dimension': 1536,\n 'index_fullness': 0.0,\n 'metric': 'cosine',\n 'namespaces': {'': {'vector_count': 4838}},\n 'total_vector_count': 4838,\n 'vector_type': 'dense'}"},"metadata":{},"execution_count":40}]},{"source":"## Task 5: Retrieval Augmented Generation","metadata":{},"id":"6b90e6fc-f627-40a2-8276-7df46b5d7155","cell_type":"markdown"},{"source":"In the previous task we built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier.\n\n### Workflow\n\n* Create a LangChain `vectorstore` object using our `index` and `embed_model`.\n* Try searching for relevant information about Llama 2.\n* Create a function (`augment_prompt`) that can take our query, retrieve information using the `vectorstore`, and merge them all into a single retrieval-augmented prompt.\n* Try asking the chatbot Llama 2 questions with and without RAG, comparing the differences.","metadata":{},"id":"946741c8-6ad8-4aa4-afe4-e3357929918b","cell_type":"markdown"},{"source":"To use LangChain's RAG pipeline we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object.","metadata":{},"id":"320fa5d8-9642-4c7f-9a9f-9665b9c20475","cell_type":"markdown"},{"source":"### Instructions\n\nInitialize the vector store object.\n\n- From the `langchain_pinecone` package, import `PineconeVectorStore`.\n- State the metadata field that contains our text (`\"chunk\"`). Assign to `text_field`.\n- Create a `PineconeVectorStore` from the index, the embedding model, and the text field. Assign to `vectorstore`.","metadata":{},"id":"f35f4d72-8e2c-43d8-883c-8ff1e91b13ab","cell_type":"markdown"},{"source":"!pip install pinecone-client langchain_openai langchain","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"3ac17c69-9087-4132-a167-4fbd0268c5f2","outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pinecone-client in /home/repl/.local/lib/python3.10/site-packages (3.2.2)\nRequirement already satisfied: langchain_openai in /home/repl/.local/lib/python3.10/site-packages (0.1.6)\nRequirement already satisfied: langchain in /home/repl/.local/lib/python3.10/site-packages (0.1.19)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2025.1.31)\nRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4 in /home/repl/.local/lib/python3.10/site-packages (from pinecone-client) (4.11.0)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.3.0)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.46 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.1.53)\nRequirement already satisfied: openai<2.0.0,>=1.24.0 in /home/repl/.local/lib/python3.10/site-packages (from langchain_openai) (1.27.0)\nRequirement already satisfied: tiktoken<1,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.38)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.12)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.7)\nRequirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.38)\nRequirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.2)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.147)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (1.33)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (23.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.15)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.8.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.7.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.3.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.2.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain_openai) (3.0.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"}],"execution_count":38},{"source":"# From the langchain_pinecone package, import PineconeVectorStore\nfrom langchain_pinecone import PineconeVectorStore\n\n# Define the metadata field that contains our text (\"chunk\"). Assign to `text_field`.\ntext_field = \"chunk\"\n\n# Create a PineconeVectorStore from the index, the embedding model, and the text field.\n# Assign to `vectorstore.\nvectorstore = PineconeVectorStore(\n    index, embed_model, text_field\n)","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1741919935815,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# From the langchain_pinecone package, import PineconeVectorStore\nfrom langchain_pinecone import PineconeVectorStore\n\n# Define the metadata field that contains our text (\"chunk\"). Assign to `text_field`.\ntext_field = \"chunk\"\n\n# Create a PineconeVectorStore from the index, the embedding model, and the text field.\n# Assign to `vectorstore.\nvectorstore = PineconeVectorStore(\n    index, embed_model, text_field\n)","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680","outputsMetadata":{"0":{"height":185,"type":"stream"}}},"id":"d91af8d9-9e0b-4246-ba92-08769b7f6017","cell_type":"code","execution_count":41,"outputs":[]},{"source":"Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Llama 2.","metadata":{},"id":"e7231db5-b65e-431c-87fb-e4600b5901e1","cell_type":"markdown"},{"source":"### Instructions\n\nPerform similarity search against a question.\n\n- Define a question. Assign to query.\n    - Use the text `\"What is so special about Llama 2?\"`.\n- Perform a similarity search for the query, returning the top 3 results.","metadata":{},"id":"e92e91fc-f4b1-437c-a8eb-e4ee507c0b19","cell_type":"markdown"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n    \nTo perform a similarity search, call the vectorstore's `.similarity_search()` method, passing the query and setting `k` to the number of results to return.\n\n</p>\n</details>","metadata":{},"id":"5bb954e1-f283-4310-a891-9250bdf62f01","cell_type":"markdown"},{"source":"# Define a new question\nquery = \"What is so special about Llama 2?\"\n\n# Use similarity search, returning the top 3 results\nvectorstore.similarity_search(query, k=3)","metadata":{"executionCancelledAt":null,"executionTime":295,"lastExecutedAt":1741920067066,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define a new question\nquery = \"What is so special about Llama 2?\"\n\n# Use similarity search, returning the top 3 results\nvectorstore.similarity_search(query, k=3)","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"11343325-0736-4b08-b85b-1d4735a0402d","cell_type":"code","execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":"[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n Document(page_content='Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"},"metadata":{},"execution_count":42}]},{"source":"We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier.","metadata":{},"id":"a4c5d7a5-9429-4d85-b8a5-f85aa9ae8654","cell_type":"markdown"},{"source":"### Instructions\n\nRun the code to define a function to augment a prompt with knowledge base results.","metadata":{},"id":"ed80b668-6796-4795-8138-711e0325f050","cell_type":"markdown"},{"source":"# Define this function to augment the prompt with data from the vector database\ndef augment_prompt(query: str):\n    results = vectorstore.similarity_search(query, k=3)\n    source_knowledge = \"\\n\".join([x.page_content for x in results])\n    augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\nthe contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\nContexts:\n{source_knowledge}\n\nQuery: {query}\"\"\"\n    return augmented_prompt","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1741920107635,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define this function to augment the prompt with data from the vector database\ndef augment_prompt(query: str):\n    results = vectorstore.similarity_search(query, k=3)\n    source_knowledge = \"\\n\".join([x.page_content for x in results])\n    augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\nthe contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\nContexts:\n{source_knowledge}\n\nQuery: {query}\"\"\"\n    return augmented_prompt","lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"19b86b04-9630-4fb8-a4f9-9f5b38300623","cell_type":"code","execution_count":43,"outputs":[]},{"source":"Using this we produce an augmented prompt:","metadata":{},"id":"b4a4f1a7-5d0b-4e7f-ab38-fbada51ba1d8","cell_type":"markdown"},{"source":"# Print the augmented prompt for the query about Llama 2\naugment_prompt(query)","metadata":{"executionCancelledAt":null,"executionTime":213,"lastExecutedAt":1741920228696,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Print the augmented prompt for the query about Llama 2\naugment_prompt(query)","outputsMetadata":{"0":{"height":579,"type":"stream"}},"lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"0ba045f5-88d7-4ce3-9ae6-e96cafa96e02","cell_type":"code","execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'Using the contexts below, answer the query. If some information is not provided within\\nthe contexts below, do not include, and if the query cannot be answered with the below information, say \"I don\\'t know\".\\n\\nContexts:\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint\\n\\nQuery: What is so special about Llama 2?'"},"metadata":{},"execution_count":45}]},{"source":"There is still a lot of text here, so let's pass it onto our chat model to see how it performs.","metadata":{},"id":"5be7c70d-a1c2-414b-b09b-afbc046503df","cell_type":"markdown"},{"source":"### Instructions\n\nAsk GPT about LLama2, augmenting the prompt with source knowledge from the Pinecone vector index.\n\n- Create a new human message. Assign to `prompt`.\n    - Call `augment_prompt()` on the query and use this as the content.\n- Append the prompt to `messages`.\n- Send the messages to GPT. Assign to `res`.\n- Print the contents of the response.","metadata":{},"id":"bafc0318-25bc-49f6-b9d8-aed3683ee2c4","cell_type":"markdown"},{"source":"# Define the augmented prompt as a human message. Assign to prompt.\nprompt = HumanMessage(content=augment_prompt(query))\n\n# Append the prompt the the list of messages\nmessages.append(prompt)\n\n# Invoke a chat with the list of messages\nres = chat.invoke(messages)\n\n# Print the contents of the response\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":1865,"lastExecutedAt":1741920384200,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define the augmented prompt as a human message. Assign to prompt.\nprompt = HumanMessage(content=augment_prompt(query))\n\n# Append the prompt the the list of messages\nmessages.append(prompt)\n\n# Invoke a chat with the list of messages\nres = chat.invoke(messages)\n\n# Print the contents of the response\nprint(res.content)","outputsMetadata":{"0":{"height":143,"type":"stream"}},"lastExecutedByKernel":"9a31b3cc-ab20-441f-a879-d20d96ac0680"},"id":"6f70d2a0-d5af-448b-8f00-0aec873ac250","cell_type":"code","execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":"Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) with parameter scales ranging from 7 billion to 70 billion parameters. These fine-tuned LLMs, such as L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases. They have been shown to outperform open-source chat models on most benchmarks tested and have demonstrated strong performance in terms of helpfulness and safety based on human evaluations. This suggests that Llama 2 models may serve as suitable substitutes for closed-source models in certain applications. The development and release of Llama 2 represent advancements in AI alignment research and provide a transparent and reproducible approach to fine-tuning and safety considerations in language models.\n"}]},{"source":"We can continue with more Llama 2 questions. Let's try _without_ RAG first:","metadata":{},"id":"f3547c02-31ea-4847-8042-7095d22a0fe9","cell_type":"markdown"},{"source":"### Instructions\n\nAsk GPT about LLama 2.\n\n- Create a new human message. Assign to `prompt`.\n    - Use the context `\"What safety measures were used in the development of llama 2?\"`.\n- Invoke a chat with GPT sending the messages plus the prompt. Assign to `res`.\n    - *Don't use `.append()` here, as we don't want to store the latest message in the conversation. The prompt needs to be converted to a list to add it to the existing list.*\n- Print the contents of the response.","metadata":{},"id":"b38ac3b3-efdd-43f5-99d5-0582745e0de0","cell_type":"markdown"},{"source":"# Create a new human message. Assign to prompt.\nprompt = HumanMessage(content = \"What safety measures were used in the development of llama 2?\")\n\n# Invoke a chat with GPT sending the messages plus the prompt to GPT. \n# Assign to res. Don't use .append()!\nres = chat(messages + [prompt])\n\n# Print the contents of the response.\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":332,"type":"stream"},"1":{"height":332,"type":"stream"}},"lastExecutedByKernel":null},"id":"00e797e1-5526-416b-be41-88925fa14960","cell_type":"code","execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":"In the development of Llama 2, safety measures were implemented to ensure the usability and safety of the models optimized for dialogue use cases. The safety measures included:\n\n- Optimization for dialogue use cases: The Llama 2 LLMs were fine-tuned and optimized specifically for dialogue use cases, which likely involved training the models on datasets relevant to conversation and ensuring they perform well in dialogue scenarios.\n\n- Human evaluations for helpfulness and safety: The models were evaluated by humans to assess their helpfulness and safety. This likely involved gathering feedback from human evaluators on how well the models performed in terms of providing useful responses and maintaining a safe interaction environment.\n\n- Comparison with closed-source models: The Llama 2 models were compared with closed-source models to assess their performance and safety. This comparison likely involved evaluating how well the Llama 2 models aligned with human preferences and safety standards compared to proprietary models.\n\nOverall, these safety measures aimed to ensure that the Llama 2 models not only performed well in dialogue scenarios but also met certain standards of helpfulness and safety when interacting with users.\n"}]},{"source":"The chatbot is able to respond about Llama 2 thanks to it's conversational history stored in `messages`. However, it doesn't know anything about the safety measures themselves as we have not provided it with that information via the RAG pipeline. Let's try again but with RAG.","metadata":{},"id":"8ea55fa4-c8a5-436e-8934-8d90a4002c57","cell_type":"markdown"},{"source":"### Instructions\n\nAsk GPT about LLama 2 again.\n\n- Do the same thing again, but this time augment the prompt using `augment_prompt()`.","metadata":{},"id":"31afe145-f1de-4ddf-abba-051144d6d3bc","cell_type":"markdown"},{"source":"# Create another human message with the same question, augmenting the prompt\nprompt = HumanMessage(content = augment_prompt(\"What safety measures were used in the development of llama 2?\"))\n\n\n# Invoke a chat with the list of messages + the latest prompt\nres = chat(messages + [prompt])\n\n# Print the contents of the response\nprint(res.content)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":290,"type":"stream"}},"lastExecutedByKernel":null},"id":"412a5fb9-5bbb-46cd-8636-6bcb7f575605","cell_type":"code","execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":"In the development of Llama 2, safety measures were implemented to increase the safety of the models. These safety measures included:\n\n1. Using safety-specific data annotation and tuning: The models were annotated and tuned with safety in mind to ensure that they adhere to safety standards and considerations.\n\n2. Conducting red-teaming: Red-teaming involves simulating adversarial attacks or potential risks to identify vulnerabilities in the models and improve their robustness.\n\n3. Employing iterative evaluations: Continuous evaluations were conducted throughout the development process to assess the safety of the models and make necessary adjustments.\n\nThese safety measures were aimed at enhancing the safety of the fine-tuned LLMs and ensuring responsible development practices in the field of language model research.\n"}]},{"source":"We get a much better informed response that includes several items missing in the previous non-RAG response, such as \"red-teaming\", \"iterative evaluations\", and the intention of the researchers to share this research to help \"improve their safety, promoting responsible development in the field\".","metadata":{},"id":"e79d11dd-338b-4732-8315-7434a4e82d48","cell_type":"markdown"},{"source":"## Summary\n\nYou built a chatbot that can answer questions about cutting edge large language models!\n\nIn particular, you\n\n- learned how to have a conversation with GPT by appending messages.\n- saw how to provide context in a prompt to help GPT answer questions.\n- setup a Pinecone database and added data to a vector index.\n- retrieved text relevant to user questions.\n- combined it all to create a chatbot that answered questions that GPT could not answer by itself.","metadata":{},"id":"ecea5aba-3a90-4655-9efa-425bc880e0cc","cell_type":"markdown"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}