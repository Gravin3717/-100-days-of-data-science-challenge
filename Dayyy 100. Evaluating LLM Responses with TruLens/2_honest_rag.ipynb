{"cells":[{"source":"# Iterating on LLM Apps with TruLens\n\nOur simple RAG often struggles with retrieving not enough information from the insurance manual to properly answer the question. The information needed may be just outside the chunk that is identified and retrieved by our app. Reducing the size of the chunk and adding \"sentence windows\" to our retrieval is an advanced RAG technique that can help with retrieving more targeted, complete context. Here we can try this technique, and test its success with TruLens.","metadata":{},"id":"03ccd61b-ca55-4ff0-8481-ac07523b5029","cell_type":"markdown"},{"source":"import openai\nfrom trulens_eval import Tru\ntru = Tru(database_redact_keys=True)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"76434568-9f5d-40a7-8048-f71ccaabd1da","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\nðŸ”’ Secret keys will not be included in the database.\n"}]},{"source":"## Load data and test set","metadata":{},"id":"779f2438-60d4-4475-a2ca-206bc0fc641b","cell_type":"markdown"},{"source":"from llama_hub.smart_pdf_loader import SmartPDFLoader\n\nllmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\npdf_loader = SmartPDFLoader(llmsherpa_api_url=llmsherpa_api_url)\n\ndocuments = pdf_loader.load_data(\"https://www.iii.org/sites/default/files/docs/pdf/Insurance_Handbook_20103.pdf\")\n\n# Load some questions for evaluation\nhonest_evals = [\n    \"What are the typical coverage options for homeowners insurance?\",\n    \"What are the requirements for long term care insurance to start?\",\n    \"Can annuity benefits be passed to beneficiaries?\",\n    \"Are credit scores used to set insurance premiums? If so, how?\",\n    \"Who provides flood insurance?\",\n    \"Can you get flood insurance outside high-risk areas?\",\n    \"How much in losses does fraud account for in property & casualty insurance?\",\n    \"Do pay-as-you-drive insurance policies have an impact on greenhouse gas emissions? How much?\",\n    \"What was the most costly earthquake in US history for insurers?\",\n    \"Does it matter who is at fault to be compensated when injured on the job?\"\n]","metadata":{},"cell_type":"code","id":"b2d1577a-8476-4f6a-8702-4189d24dab67","outputs":[],"execution_count":null},{"source":"## Set up Evaluation","metadata":{},"id":"9da4f007-422c-45ca-a05c-f1e82288797f","cell_type":"markdown"},{"source":"import os\nimport numpy as np\nfrom trulens_eval import Tru, Feedback, TruLlama, OpenAI as fOpenAI\n\nfrom trulens_eval.feedback import Groundedness\n\nopenai = fOpenAI()\n\nqa_relevance = (\n    Feedback(openai.relevance_with_cot_reasons, name=\"Answer Relevance\")\n    .on_input_output()\n)\n\nqs_relevance = (\n    Feedback(openai.relevance_with_cot_reasons, name = \"Context Relevance\")\n    .on_input()\n    .on(TruLlama.select_source_nodes().node.text)\n    .aggregate(np.mean)\n)\n\n# embedding distance\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom trulens_eval.feedback import Embeddings\n\nmodel_name = 'text-embedding-ada-002'\n\nembed_model = OpenAIEmbeddings(\n    model=model_name,\n    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n)\n\nembed = Embeddings(embed_model=embed_model)\nf_embed_dist = (\n    Feedback(embed.cosine_distance)\n    .on_input()\n    .on(TruLlama.select_source_nodes().node.text)\n)\n\nfrom trulens_eval.feedback import Groundedness\n\ngrounded = Groundedness(groundedness_provider=openai)\n\nf_groundedness = (\n    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n        .on(TruLlama.select_source_nodes().node.text.collect())\n        .on_output()\n        .aggregate(grounded.grounded_statements_aggregator)\n)\n\nhonest_feedbacks = [qa_relevance, qs_relevance, f_embed_dist, f_groundedness]","metadata":{"executionCancelledAt":null,"executionTime":874,"lastExecutedAt":1701189513508,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import os\nimport numpy as np\nfrom trulens_eval import Tru, Feedback, TruLlama, OpenAI as fOpenAI\n\nfrom trulens_eval.feedback import Groundedness\n\nopenai = fOpenAI()\n\nqa_relevance = (\n    Feedback(openai.relevance_with_cot_reasons, name=\"Answer Relevance\")\n    .on_input_output()\n)\n\nqs_relevance = (\n    Feedback(openai.relevance_with_cot_reasons, name = \"Context Relevance\")\n    .on_input()\n    .on(TruLlama.select_source_nodes().node.text)\n    .aggregate(np.mean)\n)\n\n# embedding distance\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom trulens_eval.feedback import Embeddings\n\nmodel_name = 'text-embedding-ada-002'\n\nembed_model = OpenAIEmbeddings(\n    model=model_name,\n    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n)\n\nembed = Embeddings(embed_model=embed_model)\nf_embed_dist = (\n    Feedback(embed.cosine_distance)\n    .on_input()\n    .on(TruLlama.select_source_nodes().node.text)\n)\n\nfrom trulens_eval.feedback import Groundedness\n\ngrounded = Groundedness(groundedness_provider=openai)\n\nf_groundedness = (\n    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n        .on(TruLlama.select_source_nodes().node.text.collect())\n        .on_output()\n        .aggregate(grounded.grounded_statements_aggregator)\n)\n\nhonest_feedbacks = [qa_relevance, qs_relevance, f_embed_dist, f_groundedness]","outputsMetadata":{"0":{"height":185,"type":"stream"}}},"id":"779812f4-daeb-45d0-a504-eefeb5bd7166","cell_type":"code","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\nâœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\nâœ… In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\nâœ… In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\nâœ… In cosine_distance, input query will be set to __record__.main_input or `Select.RecordInput` .\nâœ… In cosine_distance, input document will be set to __record__.app.query.rets.source_nodes[:].node.text .\nâœ… In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text.collect() .\nâœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"}]},{"source":"Our simple RAG often struggles with retrieving not enough information from the insurance manual to properly answer the question. The information needed may be just outside the chunk that is identified and retrieved by our app. Let's try sentence window retrieval to retrieve a wider chunk.","metadata":{},"id":"7fd5d9b8-b5eb-4252-972c-b892fdcf1eb4","cell_type":"markdown"},{"source":"from llama_index.node_parser import SentenceWindowNodeParser\nfrom llama_index.indices.postprocessor import MetadataReplacementPostProcessor\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\nfrom llama_index import load_index_from_storage\nfrom llama_index import Document\nfrom llama_index import ServiceContext, VectorStoreIndex, StorageContext\nfrom llama_index.llms import OpenAI\nimport os\n\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n\n# knowledge store\ndocument = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n\n# set system prompt\nfrom llama_index import Prompt\nsystem_prompt = Prompt(\"We have provided context information below that you may use. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Please answer the question: {query_str}\\n\")\n\ndef build_sentence_window_index(\n    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n):\n    # create the sentence window node parser w/ default settings\n    node_parser = SentenceWindowNodeParser.from_defaults(\n        window_size=3,\n        window_metadata_key=\"window\",\n        original_text_metadata_key=\"original_text\",\n    )\n    sentence_context = ServiceContext.from_defaults(\n        llm=llm, # fill llm\n        embed_model=embed_model, # embed model\n        node_parser=node_parser, # node parser\n    )\n    if not os.path.exists(save_dir):\n        sentence_index = VectorStoreIndex.from_documents(\n            [document], service_context=sentence_context\n        )\n        sentence_index.storage_context.persist(persist_dir=save_dir)\n    else:\n        sentence_index = load_index_from_storage(\n            StorageContext.from_defaults(persist_dir=save_dir),\n            service_context=sentence_context,\n        )\n\n    return sentence_index\n\nsentence_index = build_sentence_window_index(\n    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n)\n\ndef get_sentence_window_query_engine(\n    sentence_index,\n    system_prompt,\n    similarity_top_k=6, # top k\n    rerank_top_n=2,\n):\n    # define postprocessors\n    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n    rerank = SentenceTransformerRerank(\n        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n    )\n\n    sentence_window_engine = sentence_index.as_query_engine(\n        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank], text_qa_template = system_prompt\n    )\n    return sentence_window_engine\n\nsentence_window_engine = get_sentence_window_query_engine(sentence_index, system_prompt=system_prompt)\n\ntru_recorder_rag_sentencewindow = TruLlama(\n        sentence_window_engine,\n        app_id='2) Sentence Window RAG - Honest Eval',\n        feedbacks=honest_feedbacks\n    )","metadata":{"executionCancelledAt":null,"executionTime":18481,"lastExecutedAt":1701189793026,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from llama_index.node_parser import SentenceWindowNodeParser\nfrom llama_index.indices.postprocessor import MetadataReplacementPostProcessor\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\nfrom llama_index import load_index_from_storage\nfrom llama_index import Document\nfrom llama_index import ServiceContext, VectorStoreIndex, StorageContext\nfrom llama_index.llms import OpenAI\nimport os\n\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n\n# knowledge store\ndocument = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n\n# set system prompt\nfrom llama_index import Prompt\nsystem_prompt = Prompt(\"We have provided context information below that you may use. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Please answer the question: {query_str}\\n\")\n\ndef build_sentence_window_index(\n    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n):\n    # create the sentence window node parser w/ default settings\n    node_parser = SentenceWindowNodeParser.from_defaults(\n        window_size=3,\n        window_metadata_key=\"window\",\n        original_text_metadata_key=\"original_text\",\n    )\n    sentence_context = ServiceContext.from_defaults(\n        llm=llm, # fill llm\n        embed_model=embed_model, # embed model\n        node_parser=node_parser, # node parser\n    )\n    if not os.path.exists(save_dir):\n        sentence_index = VectorStoreIndex.from_documents(\n            [document], service_context=sentence_context\n        )\n        sentence_index.storage_context.persist(persist_dir=save_dir)\n    else:\n        sentence_index = load_index_from_storage(\n            StorageContext.from_defaults(persist_dir=save_dir),\n            service_context=sentence_context,\n        )\n\n    return sentence_index\n\nsentence_index = build_sentence_window_index(\n    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n)\n\ndef get_sentence_window_query_engine(\n    sentence_index,\n    system_prompt,\n    similarity_top_k=6, # top k\n    rerank_top_n=2,\n):\n    # define postprocessors\n    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n    rerank = SentenceTransformerRerank(\n        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n    )\n\n    sentence_window_engine = sentence_index.as_query_engine(\n        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank], text_qa_template = system_prompt\n    )\n    return sentence_window_engine\n\nsentence_window_engine = get_sentence_window_query_engine(sentence_index, system_prompt=system_prompt)\n\ntru_recorder_rag_sentencewindow = TruLlama(\n        sentence_window_engine,\n        app_id='2) Sentence Window RAG - Honest Eval',\n        feedbacks=honest_feedbacks\n    )"},"id":"ac1ec265-82a3-4bce-a90d-841444dd3701","cell_type":"code","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/799 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8002383484d54fe18d289c7ec6bc529c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8677f3372c2d42edaa0d6fc530671389"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52a384e244984a479fc402c5670a6e7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f72ef5b66de4f52864361604069c39d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8c0decfba244b4697276d5097cfe392"}},"metadata":{}}]},{"source":"# Run evaluation on 10 sample questions\nwith tru_recorder_rag_sentencewindow as recording:\n    for question in honest_evals:\n        response = sentence_window_engine.query(question)","metadata":{"executionCancelledAt":null,"executionTime":54611,"lastExecutedAt":1701189865007,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Run evaluation on 10 sample questions\nwith tru_recorder_rag_sentencewindow as recording:\n    for question in honest_evals:\n        response = sentence_window_engine.query(question)","outputsMetadata":{"0":{"height":437,"type":"stream"}}},"id":"e35774a4-a42d-4a73-a1da-ba46291b7382","cell_type":"code","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"}]},{"source":"tru.get_leaderboard(app_ids=[\"1) Basic RAG - Honest Eval\", \"2) Sentence Window RAG - Honest Eval\"])","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":550,"type":"dataFrame","tableState":{}}}},"id":"ab588e13-414a-456a-96b9-9321e5e16c13","cell_type":"code","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"app_id","type":"string"},{"name":"Answer Relevance","type":"number"},{"name":"Groundedness","type":"number"},{"name":"cosine_distance","type":"number"},{"name":"Context Relevance","type":"number"},{"name":"latency","type":"number"},{"name":"total_cost","type":"number"}],"primaryKey":["app_id"],"pandas_version":"1.4.0"},"data":{"app_id":["2) Sentence Window RAG - Honest Eval","1) Basic RAG - Honest Eval"],"Answer Relevance":[1,1],"Groundedness":[1,0.4375],"cosine_distance":[0.1362235936,0.1570688193],"Context Relevance":[0.68,0.55],"latency":[4.6,4.5],"total_cost":[0.00071325,0.003316]}},"total_rows":2,"truncation_type":null},"text/plain":"                                      Answer Relevance  ...  total_cost\napp_id                                                  ...            \n2) Sentence Window RAG - Honest Eval               1.0  ...    0.000713\n1) Basic RAG - Honest Eval                         1.0  ...    0.003316\n\n[2 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Answer Relevance</th>\n      <th>Groundedness</th>\n      <th>cosine_distance</th>\n      <th>Context Relevance</th>\n      <th>latency</th>\n      <th>total_cost</th>\n    </tr>\n    <tr>\n      <th>app_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2) Sentence Window RAG - Honest Eval</th>\n      <td>1.0</td>\n      <td>1.0000</td>\n      <td>0.136224</td>\n      <td>0.68</td>\n      <td>4.6</td>\n      <td>0.000713</td>\n    </tr>\n    <tr>\n      <th>1) Basic RAG - Honest Eval</th>\n      <td>1.0</td>\n      <td>0.4375</td>\n      <td>0.157069</td>\n      <td>0.55</td>\n      <td>4.5</td>\n      <td>0.003316</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":6}]},{"source":"How does the sentence window RAG compare to our prototype? You decide!","metadata":{},"id":"b7a0e72d-1fc1-4b43-a614-a1f771c232d5","cell_type":"markdown"}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}