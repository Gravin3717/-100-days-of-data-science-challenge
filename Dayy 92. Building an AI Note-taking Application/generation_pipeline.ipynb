{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import click\n",
    "\n",
    "from rag_workshop.config import settings\n",
    "from rag_workshop.generation import create_rag_chain, get_documents_for_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(results: list[str]):\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(result[:500])\n",
    "        print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 16:04:28.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.retrievers\u001b[0m:\u001b[36mget_retriever\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mGetting retriever using 'sentence-transformers/all-MiniLM-L6-v2' on 'cpu' with 3 top results\u001b[0m\n",
      "/Users/pauliusztin/Documents/01_projects/second-brain-ai-assistant-course/workshops/rag/solution/.venv-rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2025-03-19 16:04:32.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.splitters\u001b[0m:\u001b[36mget_splitter\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mGetting splitter with chunk size: 200 and overlap: 30\u001b[0m\n",
      "\u001b[32m2025-03-19 16:04:32.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.splitters\u001b[0m:\u001b[36mget_splitter\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mGetting splitter with chunk size: 800 and overlap: 120\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "rag_chain = create_rag_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 16:09:59.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.retrievers\u001b[0m:\u001b[36mget_retriever\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mGetting retriever using 'sentence-transformers/all-MiniLM-L6-v2' on 'cpu' with 3 top results\u001b[0m\n",
      "\u001b[32m2025-03-19 16:10:01.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.splitters\u001b[0m:\u001b[36mget_splitter\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mGetting splitter with chunk size: 200 and overlap: 30\u001b[0m\n",
      "\u001b[32m2025-03-19 16:10:01.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.splitters\u001b[0m:\u001b[36mget_splitter\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mGetting splitter with chunk size: 800 and overlap: 120\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "## What Makes BERT Different?\n",
      "\n",
      "  * BERT builds upon recent work in pre-training contextual representations — including [Semi-supervised Sequence Learning](https://arxiv.org/abs/1511.01432), [Generative Pre-Training](https://blog.openai.com/language-unsupervised/), [ELMo](https://allennlp.org/elmo), and [ULMFit](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html).\n",
      "  * However, unlike these previous models, BERT is the first deeply bidirectional, unsupervised language representa\n",
      "...\n",
      "\n",
      "Document 1:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* ELMo came up with the concept of contextualized embeddings by grouping together the hidden states of the LSTM-based model (and the initial non-contextualized embedding) in a certain way (concatenation followed by weighted summation).\n",
      "\n",
      "\n",
      "\n",
      "## BERT: an Overview\n",
      "\n",
      "  * At the input, BERT (and many other transformer models) consume 512 tokens max —- truncating anything beyond this length. Since it can generate an output per input token, it can output 512 tokens.\n",
      "  * BERT actually uses WordPieces as to\n",
      "...\n",
      "\n",
      "Document 2:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* How can we pre-train a model for both MLM and NSP tasks? To understand how a model can accommodate two pre-training objectives, let’s look at how they tokenize inputs.\n",
      "  * They used WordPiece for tokenization, which has a vocabulary of 30,000 tokens, based on the most frequent sub-words (combinations of characters and symbols). Special tokens such as `[CLS]` (Classification Token), `SEP` (Separator Token), and `MASK` (Masked Token) are added during the tokenization process. These tokens are ad\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "results = get_documents_for_query(\"How does BERT work?\")\n",
    "format_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT (Bidirectional Encoder Representations from Transformers) works by using a transformer-based architecture to generate deeply bidirectional contextual representations of text. It is pre-trained using two main objectives: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\\n\\n1. **Masked Language Modeling (MLM):** BERT masks a portion of the input tokens and trains the model to predict these masked tokens based on their context. This allows BERT to learn bidirectional representations, as it considers both the left and right context of a word.\\n\\n2. **Next Sentence Prediction (NSP):** BERT is also trained to understand the relationship between sentences. It does this by predicting whether a given sentence B follows sentence A in the original text, which helps the model handle tasks involving multiple sentences.\\n\\nBERT uses WordPiece tokenization, which breaks words into smaller sub-word units, and incorporates special tokens like `[CLS]` for classification tasks and `[SEP]` for separating sentences. The model can process up to 512 tokens at a time, and its architecture ensures that the `[CLS]` token captures the contextual information of the entire input sequence.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"How does BERT work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 16:11:26.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.retrievers\u001b[0m:\u001b[36mget_retriever\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mGetting retriever using 'sentence-transformers/all-MiniLM-L6-v2' on 'cpu' with 3 top results\u001b[0m\n",
      "\u001b[32m2025-03-19 16:11:28.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.splitters\u001b[0m:\u001b[36mget_splitter\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mGetting splitter with chunk size: 200 and overlap: 30\u001b[0m\n",
      "\u001b[32m2025-03-19 16:11:28.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.splitters\u001b[0m:\u001b[36mget_splitter\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mGetting splitter with chunk size: 800 and overlap: 120\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* For each token, the model computes the similarity of its QQ vector with every other token’s KK vector in the sequence. This similarity score is then normalized (typically through softmax), resulting in attention weights.\n",
      "    * These weights tell us the degree to which each token should “attend to” (or incorporate information from) other tokens.\n",
      "  * **Weighted Summation of Values (Producing Contextual Embeddings)** :\n",
      "\n",
      "    * Using the attention weights, each token creates a weighted sum over the\n",
      "...\n",
      "\n",
      "Document 1:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* This Jupyter notebook by Jay Alammar offers a great intro to using a pre-trained BERT model to carry out sentiment classification using the Stanford Sentiment Treebank (SST2) dataset.\n",
      "\n",
      "\n",
      "\n",
      "[![](../../../images/read/bert_first.jpg)](https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb)\n",
      "\n",
      "## Further Reading\n",
      "\n",
      "  * [Generating word embeddings from BERT](https://medium.com/@dhartidhami/understanding-bert-word-embeddings-7dc4d\n",
      "...\n",
      "\n",
      "Document 2:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "* The main component passed between layers is a set of token embeddings, which are contextualized representations of each token in the sequence up to that layer.\n",
      "    * For a sequence of nn tokens, if the embedding dimension is dd, the output of each layer is an n×dn×d matrix, where each row represents the embedding of a token, now updated with contextual information learned from the previous layer.\n",
      "    * Each embedding at this point reflects the token’s meaning as influenced by the other tokens \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "results = get_documents_for_query(\"How are similarity scores normalized?\")\n",
    "format_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The similarity scores are normalized typically through softmax.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"How are similarity scores normalized?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 16:11:46.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.retrievers\u001b[0m:\u001b[36mget_retriever\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mGetting retriever using 'sentence-transformers/all-MiniLM-L6-v2' on 'cpu' with 3 top results\u001b[0m\n",
      "\u001b[32m2025-03-19 16:11:48.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.splitters\u001b[0m:\u001b[36mget_splitter\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mGetting splitter with chunk size: 200 and overlap: 30\u001b[0m\n",
      "\u001b[32m2025-03-19 16:11:48.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_workshop.splitters\u001b[0m:\u001b[36mget_splitter\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mGetting splitter with chunk size: 800 and overlap: 120\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[more](#)\n",
      "\n",
      "Cookie duration resets each session. \n",
      "\n",
      "[View details](#) | [Storage details](#) | [Privacy policy](#)\n",
      "\n",
      "Consent\n",
      "\n",
      "## Localsensor B.V.\n",
      "\n",
      "Doesn't use cookies.\n",
      "\n",
      "Data collected and processed: IP addresses, Device characteristics, Device identifiers, Non-precise location data, Precise location data, Privacy choices\n",
      "\n",
      "[more](#)\n",
      "\n",
      "Uses other forms of storage.\n",
      "\n",
      "[View details](#) | [Privacy policy](#)\n",
      "\n",
      "Consent\n",
      "\n",
      "## Online Solution\n",
      "\n",
      "Cookie duration: 365 (days).\n",
      "\n",
      "Data collected and processed: IP addre\n",
      "...\n",
      "\n",
      "Document 1:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[more](#)\n",
      "\n",
      "Cookie duration resets each session. Uses other forms of storage.\n",
      "\n",
      "[View details](#) | [Privacy policy](#)\n",
      "\n",
      "Consent\n",
      "\n",
      "## HUMAN\n",
      "\n",
      "Doesn't use cookies.\n",
      "\n",
      "Data collected and processed: IP addresses, Device characteristics, Device identifiers, Probabilistic identifiers, Non-precise location data\n",
      "\n",
      "[more](#)\n",
      "\n",
      "[View details](#) | [Privacy policy](#)\n",
      "\n",
      "Legitimate interest\n",
      "\n",
      "## Blendee srl\n",
      "\n",
      "Cookie duration: 366 (days).\n",
      "\n",
      "Data collected and processed: IP addresses, Device characteristics, Device iden\n",
      "...\n",
      "\n",
      "Document 2:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[more](#)\n",
      "\n",
      "Uses other forms of storage.\n",
      "\n",
      "[View details](#) | [Privacy policy](#)\n",
      "\n",
      "Consent\n",
      "\n",
      "## Google Advertising Products\n",
      "\n",
      "Cookie duration: 396 (days).\n",
      "\n",
      "Data collected and processed: IP addresses, Device characteristics, Device identifiers, Authentication-derived identifiers, Browsing and interaction data, User-provided data, Non-precise location data, Users’ profiles, Privacy choices\n",
      "\n",
      "[more](#)\n",
      "\n",
      "Uses other forms of storage.\n",
      "\n",
      "[View details](#) | [Storage details](#) | [Privacy policy](#)\n",
      "\n",
      "Consen\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "results = get_documents_for_query(\"What is the difference between dynamic and continuous batching?\")\n",
    "format_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I DON'T KNOW\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is the difference between dynamic and continuous batching?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
