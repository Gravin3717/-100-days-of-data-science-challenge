{"cells":[{"source":"# Getting Started with the OpenAI API and ChatGPT","metadata":{},"id":"b60709df-143c-47af-b396-96313fc1aefa","cell_type":"markdown"},{"source":"While chatting with the GPT AI is commonly done via [the web interface](https://chat.openai.com), today we are looking at using the API. This is great for programming and automation workflows (some ideas later). We'll cover:\n\n- Calling the chat functionality in the OpenAI API.\n- Extracting the response text.\n- Holding a longer conversation.\n- Combining the OpenAI API with other APIs.","metadata":{},"id":"0c5dc78a-2c33-4b6e-a3b8-9dc7a61d29ac","cell_type":"markdown"},{"source":"## Task 0: Setup","metadata":{},"id":"4b339dfc-8612-4ca1-9613-4f030f479b6a","cell_type":"markdown"},{"source":"To use GPT, we need to import the `os` and `openai` packages, and some functions from `IPython.display` to render Markdown. A later task will also use Yahoo! Finance data via the `yfinance` package.\n\nWe also need to put the environment variable we just created in a place that the `openai` package can see it.","metadata":{},"id":"4e144991-0b50-4a2b-9ab7-708e34ff53d0","cell_type":"markdown"},{"source":"### Instructions\n\n- Import the `os` package.\n- Import the `openai` package.\n- Import the `yfinance` package with the alias `yf`.\n- From the `IPython.display` package, import `display` and `Markdown`.\n- Set `openai.api_key` to the `OPENAI` environment variable.","metadata":{},"id":"d85e559b-6e89-424e-8bf7-274d7c951435","cell_type":"markdown"},{"source":"# Import the os package\nimport os\n\n# Import the openai package\nimport openai\n\n# Import yfinance as yf\nimport yfinance as yf\n\n# From the IPython.display package, import display and Markdown\nfrom IPython.display import display, Markdown\n\n# Set openai.api_key to the OPENAI environment variable\nopenai.api_key = os.environ[\"OPENAI\"]","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Import the os package\nimport os\n\n# Import the openai package\nimport openai\n\n# From the IPython.display package, import display and Markdown\nfrom IPython.display import display, Markdown\n\n# Import yfinance as yf\nimport yfinance as yf\n\n# Set openai.api_key to the OPENAI environment variable\nopenai.api_key = os.environ[\"OPENAI\"]"},"id":"6b8a7596-d34b-419b-baf3-004e48f1aca5","cell_type":"code","execution_count":3,"outputs":[]},{"source":"## Task 1: Get GPT to create a dataset","metadata":{},"id":"1ee98f37-16cf-4cf4-80f6-ac5fa2c666b5","cell_type":"markdown"},{"source":"It's time to chat! Having a conversation with GPT involves a single function call of this form.\n\n```python\nresponse = openai.ChatCompletion.create(\n    model=\"MODEL_NAME\",\n    messages=[\n        {\"role\": \"system\", \"content\": 'SPECIFY HOW THE AI ASSISTANT SHOULD BEHAVE'},\n        {\"role\": \"user\", \"content\": 'SPECIFY WANT YOU WANT THE AI ASSISTANT TO SAY'}\n    ]\n)\n```\n\nThere are a few things to unpack here.\n\nThe model names are listed in the [Model Overview](https://platform.openai.com/docs/models/overview) page of the developer documentation. Today we'll be using `gpt-3.5-turbo`, which is the latest model used by ChatGPT that has broad public API access. \n\nIf you have access to GPT-4, you can use that instead by setting `model=\"gpt-4\"`, though note that the price is 15 times higher.\n\nThere are three types of message, documented in the [Introduction](https://platform.openai.com/docs/guides/chat/introduction) to the Chat documentation:\n\n- `system` messages describe the behavior of the AI assistant. If you don't know what you want, try \"You are a helpful assistant\".\n- `user` messages describe what you want the AI assistant to say. We'll cover examples of this today.\n- `assistant` messages describe previous responses in the conversation. We'll cover how to have an interactive conversation in later tasks. \n\nThe first message should be a system message. Additional messages should alternate between user and assistant.","metadata":{},"id":"9e1baa5c-0e61-41ad-b757-aab82756d8f2","cell_type":"markdown"},{"source":"### Pro Tip\n\nGPT-4 is more \"steerable\" than GPT-3.5-turbo. That means that it can play a wider range of roles more convincingly. For API usage, it means that the system message has a larger effect on the output conversation in GPT-4 compared to GPT-3.5-turbo.\n\n","metadata":{},"id":"30b9a9a7-7ff3-4361-8b1d-ba24d10ae983","cell_type":"markdown"},{"source":"### Pro tip\n\nIf you are worried about the price of API calls, you can also set a [`max_tokens`](https://platform.openai.com/docs/api-reference/chat/create#chat/create-max_tokens) argument to limit the amount of output created.","metadata":{},"id":"b6d4d580-ceb2-49cc-8a91-6206fe198f41","cell_type":"markdown"},{"source":"### Instructions\n\n- Define the system message, `system_msg` as\n\n> 'You are a helpful assistant who understands data science.'\n\n- Define the user message, `user_msg` as: \n\n> 'Create a small dataset of data about people. The format of the dataset should be a data frame with 5 rows and 3 columns. The columns should be called \"name\", \"height_cm\", and \"eye_color\". The \"name\" column should contain randomly chosen first names. The \"height_cm\" column should contain randomly chosen heights, given in centimeters. The \"eye_color\" column should contain randomly chosen eye colors, taken from a choice of \"brown\", \"blue\", and \"green\". Provide Python code to generate the dataset, then provide the output in the format of a markdown table.'\n\n- Ask GPT to create a dataset using the `gpt-3.5-turbo` model. Assign to `response`.\n","metadata":{},"id":"437acbc2-db60-4feb-ac7f-19b50749d297","cell_type":"markdown"},{"source":"# Define the system message\nsystem_msg = 'You are a helpful assistant who understands data science'\n\n# Define the user message\nuser_msg = 'Create a small dataset of data about people. The format of the dataset should be a data frame with 5 rows and 3 columns. The columns should be called \"name\", \"height_cm\", and \"eye_color\". The \"name\" column should contain randomly chosen first names. The \"height_cm\" column should contain randomly chosen heights, given in centimeters. The \"eye_color\" column should contain randomly chosen eye colors, taken from a choice of \"brown\", \"blue\", and \"green\". Provide Python code to generate the dataset, then provide the output in the format of a markdown table.'\n\n# Create a dataset using GPT\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_msg},\n        {\"role\": \"user\", \"content\": user_msg}\n    ]\n)","metadata":{"executionTime":9140,"lastSuccessfullyExecutedCode":"# Define the system message\nsystem_msg = 'You are a helpful assistant who understands data science'\n\n# Define the user message\nuser_msg = 'Create a small dataset of data about people. The format of the dataset should be a data frame with 5 rows and 3 columns. The columns should be called \"name\", \"height_cm\", and \"eye_color\". The \"name\" column should contain randomly chosen first names. The \"height_cm\" column should contain randomly chosen heights, given in centimeters. The \"eye_color\" column should contain randomly chosen eye colors, taken from a choice of \"brown\", \"blue\", and \"green\". Provide Python code to generate the dataset, then provide the output in the format of a markdown table.'\n\n# Create a dataset using GPT\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_msg},\n        {\"role\": \"user\", \"content\": user_msg}\n    ]\n)"},"id":"c8c671da-51d6-4d7d-9769-393312ebb8cd","cell_type":"code","execution_count":4,"outputs":[]},{"source":"## Task 2: Check the response is OK","metadata":{},"id":"d818b7bc-1891-4908-9ed3-81ecfe65f20b","cell_type":"markdown"},{"source":"API calls are \"risky\" because problems can occur outside of your notebook, like internet connectivity issues, or a problem with the server sending you data, or because you ran out of API credit. You should check that the response you get is OK.\n\nGPT models return a status code with one of four values, documented in the [Response format](https://platform.openai.com/docs/guides/chat/response-format) section of the Chat documentation.\n\n- `stop`: API returned complete model output\n- `length`: Incomplete model output due to max_tokens parameter or token limit\n- `content_filter`: Omitted content due to a flag from our content filters\n- `null`: API response still in progress or incomplete\n\nThe GPT API sends data to Python in JSON format, so the response variable contains deeply nested lists and dictionaries. It's a bit of a pain to work with!\n\nFor a response variable named `response`, the status code is stored in `response[\"choices\"][0][\"finish_reason\"]`.","metadata":{},"id":"0aef6080-50bb-45b0-8b0a-62e26cd7cd08","cell_type":"markdown"},{"source":"### Pro tip\n\nIf you prefer to work with dataframes rather than nested lists and dictionaries, you can flatten the output to a single row dataframe with the following code.\n\n```python\nimport pandas as pd\npd.json_normalize(response, \"choices\", ['id', 'object', 'created', 'model', 'usage'])\n```","metadata":{},"id":"0d4b818b-186e-4b24-b461-6ed465565584","cell_type":"markdown"},{"source":"### Instructions\n\n- Check the status code of the `response` variable.","metadata":{},"id":"05ebd19c-ecbc-4cf1-853e-6563222de7f1","cell_type":"markdown"},{"source":"# Check the status code of the response variable\nresponse[\"choices\"][0][\"finish_reason\"]","metadata":{"executionTime":32,"lastSuccessfullyExecutedCode":"# Check the status code of the response variable\nresponse[\"choices\"][0][\"finish_reason\"]"},"id":"8fc5d778-266e-4858-b36f-6be9e6359569","cell_type":"code","execution_count":5,"outputs":[]},{"source":"## Task 3: Extract the AI assistant's message","metadata":{},"id":"1084f3d6-0006-459c-bf9a-f071a2d3ec5a","cell_type":"markdown"},{"source":"Buried within the response variable is the text we asked GPT to generate. Luckily, it's always in the same place.\n\n`response[\"choices\"][0][\"message\"][\"content\"]`\n\nThe response content can be printed as usual with `print(content)`, but it's Markdown content, which Jupyter notebooks can render, via `display(Markdown(content))`.","metadata":{},"id":"9d74095e-10e9-40fe-97bc-5d9f89a576b3","cell_type":"markdown"},{"source":"### Instructions\n\n- Print the content generated by GPT.\n\n- Render the Markdown content generated by GPT.\n\n- Read the code that was generated. Does it look correct?\n\n- Read the dataset that was generated. Does it match the specifications?","metadata":{},"id":"0b7ae748-08d4-435d-916c-8ec618e56324","cell_type":"markdown"},{"source":"# Print the content generated by GPT.\nprint(response[\"choices\"][0][\"message\"][\"content\"])","metadata":{"executionTime":29,"lastSuccessfullyExecutedCode":"# Print the content generated by GPT.\nprint(response[\"choices\"][0][\"message\"][\"content\"])"},"id":"aaa8badc-121b-4e40-a4f9-d14ded814905","cell_type":"code","execution_count":6,"outputs":[]},{"source":"# Render the Markdown content generated by GPT\ndisplay(Markdown(response[\"choices\"][0][\"message\"][\"content\"]))","metadata":{"executionTime":50,"lastSuccessfullyExecutedCode":"# Render the Markdown content generated by GPT\ndisplay(Markdown(response[\"choices\"][0][\"message\"][\"content\"]))"},"id":"609d3d89-2abe-4962-8d2f-a5e6a175d501","cell_type":"code","execution_count":7,"outputs":[]},{"source":"## (Not a task): Use a helper function","metadata":{},"id":"c776c0b9-05c8-4e75-a2fa-53147b7752e9","cell_type":"markdown"},{"source":"You need to write a lot of repetitive boilerplate code to do these three simple things. Having a wrapper function to abstract away the boring bits is useful. That way we can focus on data science use cases.\n\nHopefully OpenAI will improve the interface to their Python package so this sort of thing is built-in. In the meantime, feel free to use this in your own code.\n\nThe function takes 2 arguments.\n\n- `system`: A string containing the system message.\n- `user_assistant`: An array of strings that alternate user message then assistant message.\n\nThe return value is the generated content.","metadata":{},"id":"791988c1-13e8-45d4-b7a4-56402260c535","cell_type":"markdown"},{"source":"### Instructions\n\n- Run the next cell so you have access to the function.","metadata":{},"id":"297316fe-89f4-4e7e-98db-1ed6c86dda0a","cell_type":"markdown"},{"source":"def chat(system, user_assistant):\n    assert isinstance(system, str), \"`system` should be a string\"\n    assert isinstance(user_assistant, list), \"`user_assistant` should be a list\"\n    system_msg = [{\"role\": \"system\", \"content\": system}]\n    user_assistant_msgs = [\n        {\"role\": \"assistant\", \"content\": user_assistant[i]} if i % 2 else {\"role\": \"user\", \"content\": user_assistant[i]} \n        for i in range(len(user_assistant))\n    ]\n    msgs = system_msg + user_assistant_msgs\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=msgs\n    )\n    status_code = response[\"choices\"][0][\"finish_reason\"]\n    assert status_code == \"stop\", f\"The status code was {status_code}.\"\n    return response[\"choices\"][0][\"message\"][\"content\"]\n        ","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"def chat(system, user_assistant):\n    assert isinstance(system, str), \"`system` should be a string\"\n    assert isinstance(user_assistant, list), \"`user_assistant` should be a list\"\n    system_msg = [{\"role\": \"system\", \"content\": system}]\n    user_assistant_msgs = [\n        {\"role\": \"assistant\", \"content\": user_assistant[i]} if i % 2 else {\"role\": \"user\", \"content\": user_assistant[i]} \n        for i in range(len(user_assistant))\n    ]\n    msgs = system_msg + user_assistant_msgs\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=msgs\n    )\n    status_code = response[\"choices\"][0][\"finish_reason\"]\n    assert status_code == \"stop\", f\"The status code was {status_code}.\"\n    return response[\"choices\"][0][\"message\"][\"content\"]\n        "},"id":"7c230c3b-0442-4f10-8e0e-a55c5829e8a5","cell_type":"code","execution_count":8,"outputs":[]},{"source":"Here is a check to make sure the function works.","metadata":{},"id":"43f72850-3b02-416d-b9f3-70a70d287861","cell_type":"markdown"},{"source":"response_fn_test = chat(\n    \"You are a machine learning expert who writes tersely.\", \n    [\"Explain what a support vector machine model is.\"]\n)\ndisplay(Markdown(response_fn_test))","metadata":{"executionTime":5274,"lastSuccessfullyExecutedCode":"response_fn_test = chat(\n    \"You are a machine learning expert who writes tersely.\", \n    [\"Explain what a support vector machine model is.\"]\n)\ndisplay(Markdown(response_fn_test))"},"id":"20e58a34-7c09-44e3-aa60-56cf1954c182","cell_type":"code","execution_count":9,"outputs":[]},{"source":"### Pro Tip\n\nIn the system message for that check to make sure the function works correctly, I told the AI that it \"writes tersely\". This reduces the amount of output it generates, saving you some credits. You won't always want a terse output, but it's useful if you are just testing things.\n\n**When you don't care too much about the style of the output, include a command to \"write tersely\" in the system message.**","metadata":{},"id":"8078549e-771a-4174-a992-7290a1acd2e3","cell_type":"markdown"},{"source":"## Task 4: Perform a calculation by reusing messages","metadata":{},"id":"c47d7d28-7882-4efe-91f5-13e0f55f00fc","cell_type":"markdown"},{"source":"The \"zero-shot\" case where the AI gives you the perfect response the first time is pretty rare. As with humans, you often need to have a longer conversation. This is where the user message-assistant message alternation comes in handy.","metadata":{},"id":"ccc45736-8708-49cb-95da-24f81de15c36","cell_type":"markdown"},{"source":"### Instructions\n\n- Assign the content from the response in Task 1 to `assistant_msg`.\n\n- Define a new user message, `user_msg2` as follows.\n\n> 'Using the dataset you just created, write code to calculate the mean of the `height_cm` column. Also include the result of the calculation.'\n\n- Create a list of user and assistant messages from `user_msg`, `assistant_msg`, and `user_msg2`. Assign to `user_assistant_msgs`.\n\n- Get GPT to perform the request, using `system_msg` (from Task 1) and `user_assistant_msgs`. Assign to `response_calc`.\n\n- Read the code that was generated. Does it look correct?\n\n- Read the answer that was generated. Does it look correct?","metadata":{},"id":"39462527-da47-45c0-831c-494867a4c698","cell_type":"markdown"},{"source":"# Assign the content from the response in Task 1 to assistant_msg\nassistant_msg = response[\"choices\"][0][\"message\"][\"content\"]\n\n# Define a new user message\nuser_msg2 = 'Using the dataset you just created, write code to calculate the mean of the `height_cm` column. Also include the result of the calculation.'\n\n# Create an array of user and assistant messages\nuser_assistant_msgs = [user_msg, assistant_msg, user_msg2]\n\n# Get GPT to perform the request\nresponse_calc = chat(system_msg, user_assistant_msgs)\n\n# Display the generated content\ndisplay(Markdown(response_calc))","metadata":{"executionTime":7524,"lastSuccessfullyExecutedCode":"# Assign the content from the response in Task 1 to assistant_msg\nassistant_msg = response[\"choices\"][0][\"message\"][\"content\"]\n\n# Define a new user message\nuser_msg2 = 'Using the dataset you just created, write code to calculate the mean of the `height_cm` column. Also include the result of the calculation.'\n\n# Create an array of user and assistant messages\nuser_assistant_msgs = [user_msg, assistant_msg, user_msg2]\n\n# Get GPT to perform the request\nresponse_calc = chat(system_msg, user_assistant_msgs)\n\n# Display the generated content\ndisplay(Markdown(response_calc))"},"id":"869fb5f0-4f08-4d7e-98f2-3e9162eb1828","cell_type":"code","execution_count":10,"outputs":[]},{"source":"### Pro Tip\n\nYou don't have to use real assistant responses in your conversation. You can make them up yourself! This has two advantages:\n\n1. It saves on API calls, so it's cheaper.\n2. You can provide you ideal assistant response, which can improve future output in the conversation.\n\n**It can be helpful to include fake assistant messages at the start of the conversation.**","metadata":{},"id":"a0224396-8491-408c-a3ac-89e59e9a97b5","cell_type":"markdown"},{"source":"### Pro Tip\n\nAlthough the official guidance is to include the system message at the start, some users have reported that repeating the system message later in the conversation prevents GPT \"forgetting\" its role.\n\n**For longer conversations, experiment with repeating the system message.**","metadata":{},"id":"a60757ff-fa68-47e9-b672-24fe6ec645cb","cell_type":"markdown"},{"source":"## (Not a task): Why should you care about the API?","metadata":{},"id":"f7ceafc5-ee51-4301-9163-43a602094887","cell_type":"markdown"},{"source":"At this point, you know pretty much everything about how to use the OpenAI API to generate content with GPT. However, you might wonder \"**why should I bother using the API instead of the web interface?**\".\n\nAPIs are great for automation in data pipelines or inside software. Some possible data science users of the API include:\n\n- Pull in data (from a database, another API, or wherever), and ask GPT to summarize it or generate a report about it.\n- Use the [linkedin-api-client](https://pypi.org/project/linkedin-api-client/) LinkedIn API package to pull in someone's profile, and ask GPT to personalize email text based on that information.\n- Use the [scholarly](https://pypi.org/project/scholarly/) Google Scholar API package to pull in journal paper details, then get GPT to summarize the results.\n- Embed the API in a dashboard to automatically provide a text summary of the results.\n- Provide a natural language interface to your data mart.","metadata":{},"id":"4ab12485-b071-421e-a609-cc3137bc775e","cell_type":"markdown"},{"source":"## Task 5: Get Silicon Valley Bank stock data from Yahoo! Finance","metadata":{},"id":"28dd3942-36aa-4cba-be1a-bc78d4015e95","cell_type":"markdown"},{"source":"Lets try an example of automatically analyzing some stock data. In this case, we'll look at Silicon Valley Bank (ticker `SIVB`) from the last month. The data is available from Yahoo! Finance, and can be imported into Python via the `yfinance` package.\n\nTo get recent stock history for the last `N` months, the code pattern is\n\n```python\nticker = yf.Ticker(\"TICKERNAME\")\nticker_history = ticker.history(period=\"Nmo\")\n```\n\nIn general, we should try to minimize the amount of data sent to the API (network traffic is slow), so we'll stick to looking at the `Close` column, which contains the stock price at the close of the day. Further, we'll round the prices to the nearest cent (2 decimal places).","metadata":{},"id":"85794705-3126-4ade-b628-4e2e76e4b278","cell_type":"markdown"},{"source":"### Instructions\n\n- Create a Ticker object for `SIVB`. Assign to `sivb`.\n\n- Get the stock history for SIVB for the period of 1 month (`\"1mo\"`). Assign to `sivb_history`.\n\n- Select the `Close` column and round it to two decimal places. Assign to `sivb_close`.","metadata":{},"id":"73e333b0-fc64-43cc-ba33-3ce62d6c8fad","cell_type":"markdown"},{"source":"# Create a Ticker object for SIVB\nsivb = yf.Ticker(\"SIVB\")\n\n# Get the stock history for SIVB for the period of 1 month\nsivb_history = sivb.history(period=\"1mo\")\n\n# Select the Close column and round it to two decimal places\nsivb_close = sivb_history[[\"Close\"]].round(2)","metadata":{"executionTime":40,"lastSuccessfullyExecutedCode":"# Create a Ticker object for SIVB\nsivb = yf.Ticker(\"SIVB\")\n\n# Get the stock history for SIVB for the period of 1 month\nsivb_history = sivb.history(period=\"1mo\")\n\n# Select the Close column and round it to two decimal places\nsivb_close = sivb_history[[\"Close\"]].round(2)"},"id":"f4d806fb-ef17-4486-8517-6c7b2c4baaf5","cell_type":"code","execution_count":11,"outputs":[]},{"source":"## Task 6: Get GPT to write a financial report","metadata":{},"id":"0e002970-5b45-4c1d-b4fe-846555a7cd46","cell_type":"markdown"},{"source":"Now we have the data, we need to ask GPT to analyze it for us.\n\nOne thing that is useful to know is that you can convert a pandas dataframe into a string using the `.to_string()` method.","metadata":{},"id":"8a0d113a-128d-45a5-948a-ef4314290c0e","cell_type":"markdown"},{"source":"### Instructions\n\n- Define a system message, `system_msg_sivb`, as:\n\n> 'You are a financial data expert who writes tersely.'\n\n- Define a user message, `user_msg_sivb`, as:\n\n> '''The closing prices for the Silicon Valley Bank stock (ticker SIVB) are provided below. Provide Python code to analyze the data including the following metrics:\n> \n> - The date of the highest closing price.\n> - The date of the lowest closing price.\n> - The date with the largest change from the previous closing price.\n> \n> Also write a short report that includes the results of the calculations.\n> \n> Here is the dataset:\n> \n> '''\n\n- Append `sivb_close`, converted to a string, to the user message.\n\n- Get GPT to generate a response from `system_msg_sivb` and `user_msg_sivb`. Assign to `response_sivb`.\n\n- Render the response as Markdown.\n\n- Read the code that was generated. Does it look correct?\n\n- Read the report that was generated. Does it look correct?","metadata":{},"id":"6e63bc3e-4956-4170-ba7b-c8ccaac3dd4f","cell_type":"markdown"},{"source":"# Define a system message\nsystem_msg_sivb = 'You are a financial data expert who writes tersely.'\n\n# Define a user message (including the dataset)\nuser_msg_sivb = '''The closing prices for the Silicon Valley Bank stock (ticker SIVB) are provided below. Provide Python code to analyze the data including the following metrics:\n\n- The date of the highest closing price.\n- The date of the lowest closing price.\n- The date with the largest change from the previous closing price.\n\nAlso write a short report that includes the results of the calculations.\n\nHere is the dataset:\n\n''' + sivb_close.to_string()\n\n# Get GPT to generate a response\nresponse_sivb = chat(system_msg_sivb, [user_msg_sivb])\n\n# Render the response as Markdown\ndisplay(Markdown(response_sivb))","metadata":{"executionTime":16045,"lastSuccessfullyExecutedCode":"# Define a system message\nsystem_msg_sivb = 'You are a financial data expert who writes tersely.'\n\n# Define a user message (including the dataset)\nuser_msg_sivb = '''The closing prices for the Silicon Valley Bank stock (ticker SIVB) are provided below. Provide Python code to analyze the data including the following metrics:\n\n- The date of the highest closing price.\n- The date of the lowest closing price.\n- The date with the largest change from the previous closing price.\n\nAlso write a short report that includes the results of the calculations.\n\nHere is the dataset:\n\n''' + sivb_close.to_string()\n\n# Get GPT to generate a response\nresponse_sivb = chat(system_msg_sivb, [user_msg_sivb])\n\n# Render the response as Markdown\ndisplay(Markdown(response_sivb))"},"id":"8ad81e73-91b8-42ee-9674-ba2054bfc0da","cell_type":"code","execution_count":12,"outputs":[]},{"source":"## Keep on learning!","metadata":{},"id":"85447cff-9d76-41a1-91b1-e9d7fdf52449","cell_type":"markdown"},{"source":"For more prompt ideas, check out the [ChatGPT cheat sheet](https://www.datacamp.com/cheat-sheet/chatgpt-cheat-sheet-data-science) and take the [Introduction to ChatGPT](http://bit.ly/3TWf95Y) course.\n\nTo learn more about working with APIs, read the [Web APIs, Python Requests & Performing an HTTP Request in Python](http://bit.ly/42WR9UG) tutorial and take the [Intermediate Importing Data in Python](http://bit.ly/3G2mMC4) course.","metadata":{},"id":"a0f511fa-5ad8-4c75-9375-b85f2fadc953","cell_type":"markdown"}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}