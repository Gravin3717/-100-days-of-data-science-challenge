# Day 80. 100 Days of Data Science Challenge - 04/21/2025

## üé® Conditional Diffusion Model from Scratch: Fashion MNIST Generator ‚ú® 

[![PyTorch](https://img.shields.io/badge/PyTorch-USED-orange?logo=pytorch&style=flat-square)](https://pytorch.org/)
[![TorchMultimodal](https://img.shields.io/badge/TorchMultimodal-DiffusionLabs-blueviolet?style=flat-square)](https://github.com/facebookresearch/multimodal)
[![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python&style=flat-square)](https://www.python.org/)
[![Dataset](https://img.shields.io/badge/Dataset-FashionMNIST-yellow?style=flat-square)](https://github.com/zalandoresearch/fashion-mnist)
[![100DaysOfCode](https://img.shields.io/badge/100DaysOfDataScience-Day_80-brightgreen?style=flat-square)](https://www.100daysofcode.com/)

![Generated Boots](https://cdn.mathpix.com/cropped/2025_04_22_d520342e8b849e5cd87fg-5.jpg?height=504&width=504&top_left_y=165&top_left_x=208)

*Sample "Boot" images generated by the trained diffusion model.*

## üéØ Project Goal: Building a Generative AI Model for Fashion Items

Welcome to Day 80 of my #100DaysOfDataScience journey! Today's project dives deep into the fascinating world of **Generative AI** by building a **Conditional Denoising Diffusion Probabilistic Model (DDPM)** from scratch using **PyTorch** and the powerful `torchmultimodal.diffusion_labs` library.

**The mission?** To train a model capable of generating realistic images of specific fashion items (like boots, purses, sneakers) based on a given class label from the **Fashion MNIST dataset**. This goes beyond simple generation; it's about **controlled, conditional image synthesis**.

**What you'll see:**
*   The step-by-step construction of a diffusion model pipeline.
*   Implementation of a custom **U-Net** architecture tailored for this task.
*   Integration of **conditional embeddings** to guide the generation process.
*   Application of **Classifier-Free Guidance (CFG)** to enhance image quality.
*   The complete training loop and final image generation results.

---

## ‚ú® Key Features & Concepts Mastered

*   **Diffusion Model Fundamentals:** Implemented the core components:
    *   Noise **Schedule** (`DiscreteGaussianSchedule` with linear beta).
    *   Noise **Predictor** (`NoisePredictor`).
    *   Sampling logic (`DDPModule`).
*   **Custom U-Net Architecture:** Built a U-Net with specific down-sampling (`DownBlock`), up-sampling (`UpBlock`), and bottleneck layers, incorporating **time embeddings** and **conditional embeddings** directly into the convolutional blocks.
*   **Conditional Generation:** Trained an `nn.Embedding` layer (`encoder`) jointly with the U-Net to map fashion class labels (0-9) into vectors that guide the diffusion process.
*   **Classifier-Free Guidance (CFG):** Utilized `torchmultimodal.diffusion_labs.modules.adapters.CFGuidance` wrapper (with `guidance=2.0`) to improve sample fidelity by interpolating between conditional and unconditional model predictions during generation.
*   **Data Preparation for Diffusion:** Applied necessary transforms (`Resize`, `ToTensor`, normalization to `[-1, 1]`) and crucially used `RandomDiffusionSteps` during data loading to create noisy images (`xt`) and noise targets (`noise`) for each training step (`t`).
*   **Diffusion Loss:** Employed `DiffusionHybridLoss` for robust training, optimizing the model to predict the added noise.
*   **End-to-End Training:** Implemented a complete PyTorch training loop using `AdamW` optimizer for 25 epochs, tracking loss (`~0.03` by end).
*   **Image Generation & Interpolation:** Successfully generated images for specific classes ("boot", "purse") and even explored generating hybrid images by averaging class embeddings ("sneaker" + "boot").

![image](https://github.com/user-attachments/assets/bb00f7c4-9cf8-4c55-bfb1-6286360d174f)

---

## üõ†Ô∏è Tech Stack: The Generative AI Toolkit

*   **Core Framework:** PyTorch
*   **Diffusion Components:** `torchmultimodal-nightly` (specifically `diffusion_labs` modules: `DiscreteGaussianSchedule`, `NoisePredictor`, `DDPModule`, `CFGuidance`, `DiffusionHybridLoss`, `RandomDiffusionSteps`)
*   **Data Handling:** `torchvision` (for `FashionMNIST` dataset, transforms like `Resize`, `ToTensor`, `Lambda`)
*   **Utilities:** `tqdm` (progress bars), `torchvision.utils.make_grid`, `PIL` (via `torchvision.transforms.functional.to_pil_image`) for visualization.
*   **Environment:** Python 3.8+, CUDA-enabled GPU (training is compute-intensive!)

  ---

## üó∫Ô∏è The Blueprint: Constructing the Diffusion Model

Building a diffusion model involves several key steps, which this project implements clearly:

1.  **The Diffusion Process Conceptualized:**
    *   **Forward Process (Data Prep):** We don't explicitly simulate this step-by-step, but the `RandomDiffusionSteps` transform does the heavy lifting during data loading. It takes a clean image (`x0`), randomly picks a timestep `t`, adds the appropriate amount of Gaussian noise according to the `schedule`, and provides the noisy image (`xt`) and the original noise (`noise`) used.
    *   **Reverse Process (The Model's Goal):** The U-Net's job is to predict the noise (`noise`) that was added to get `xt` at timestep `t`, *given* the condition `c` (the fashion item class).

2.  **Defining the Noise Schedule (`DiscreteGaussianSchedule`):**
    *   We used a standard `linear_beta_schedule` over 1000 timesteps. This defines how much noise is added at each step ‚Äì starting small and increasing. The `schedule` object precomputes values needed for adding noise and for the loss calculation.

3.  **Building the Denoising Engine (Custom U-Net):**
    *   This is the core neural network. It takes the noisy image `xt` and the current timestep `t` (plus the condition `c`) as input and predicts the noise.
    *   **Architecture:** Follows the classic U-Net structure:
        *   Initial Convolution (`nn.Conv2d`).
        *   Down-sampling path (`DownBlock`): Applies convolutions and average pooling, crucially concatenating the **time embedding** and **conditional embedding** (`condition = torch.cat([timestep, condition], dim=1)`) with the image features *before* the convolutions (`self.block(torch.cat([x, c], 1))`). Stores outputs (`outs`) for skip connections.
        *   Bottleneck (`DownBlock`).
        *   Up-sampling path (`UpBlock`): Uses `nn.Upsample`, concatenates the up-sampled features with the corresponding output from the down-sampling path (`torch.cat((x_big, x), dim=1)` - skip connection!), and applies convolutions.
        *   Final convolutions (`self.prediction`, `self.variance`) to predict the noise and optionally the variance.
    *   **Time Embedding:** `nn.Embedding(steps, time_size)` converts the discrete timestep `t` into a learnable vector.
    *   **Conditional Embedding:** The separate `encoder = nn.Embedding(10, digit_size)` learns a vector for each of the 10 Fashion MNIST classes.

![image](https://github.com/user-attachments/assets/83603718-ac82-4137-8064-63d58d0c9315)

4.  **Adding Control (Conditioning & Classifier-Free Guidance):**
    *   **Conditioning:** The `encoder` provides the class embedding, which is concatenated with the time embedding and fed into the U-Net blocks. This tells the U-Net *what* kind of item to denoise towards.
    *   **CFG (`CFGuidance` wrapper):** This adapter cleverly modifies the U-Net's forward pass during *generation*. It runs the model twice: once with the condition (e.g., "boot" embedding) and once without (or with a null condition). It then extrapolates from the unconditional prediction towards the conditional one using the `guidance` scale (set to `2.0`). This often leads to sharper, more class-adherent results.

5.  **Wrapping it Up (`DDPModule`):**
    *   The `DDPModule` from `torchmultimodal` conveniently bundles the `unet` (with CFG), the `schedule`, and the `predictor` (`NoisePredictor`). It handles the iterative denoising process during *generation* based on the defined `eval_steps` (250 steps used here).

6.  **Training the Model:**
    *   **Dataset:** `FashionMNIST`, loaded with the special `RandomDiffusionSteps` transform.
    *   **Loss:** `DiffusionHybridLoss` calculates how well the U-Net's noise prediction matches the actual noise added, potentially incorporating variance prediction.
    *   **Optimization:** `AdamW` optimizer jointly trains the `UNet` parameters *and* the `encoder` embedding parameters. The loop iterates through batches, calculates loss, backpropagates, and updates weights for 25 epochs. Loss consistently decreased, indicating successful learning.

7.  **Generating Images:**
    *   Set model to `eval()` mode.
    *   Define the desired condition (e.g., "boot") using `fashion_encoder` to get the corresponding embedding `c`.
    *   Create random noise (`torch.randn`).
    *   Call the `model` (the `DDPModule`) with the noise and the conditional input `{"context": c}`. The `DDPModule` handles the multi-step denoising loop using the schedule and predictor.
    *   Visualize the resulting images after scaling back to the `[0, 1]` range.

---

## ‚ú® Generated Fashion!

The model successfully learned to generate recognizable images based on the class prompts:

**Boots:**

![Generated Boots](https://cdn.mathpix.com/cropped/2025_04_22_d520342e8b849e5cd87fg-5.jpg?height=504&width=504&top_left_y=165&top_left_x=208)

**Purses:**

![Generated Purses](https://cdn.mathpix.com/cropped/2025_04_22_d520342e8b849e5cd87fg-5.jpg?height=504&width=506&top_left_y=1167&top_left_x=204)

**Hybrid (Sneaker + Boot):**

![Generated Hybrid](https://cdn.mathpix.com/cropped/2025_04_22_d520342e8b849e5cd87fg-5.jpg?height=506&width=506&top_left_y=2174&top_left_x=204)

*(Generated by averaging the "sneaker" and "boot" embeddings)*

---

## üí° Key Learnings & Significance

*   **Diffusion Models Demystified:** This project provides a concrete, step-by-step implementation of the core DDPM concepts (schedule, U-Net, noise prediction, sampling).
*   **Power of Conditioning:** Learned how to effectively inject conditional information (class labels via embeddings) into the U-Net architecture to guide generation.
*   **CFG for Quality:** Implemented and observed the effect of Classifier-Free Guidance in improving the adherence of generated samples to the conditioning signal.
*   **TorchMultimodal Helpers:** Leveraged `diffusion_labs` components (`CFGuidance`, `DDPModule`, `DiffusionHybridLoss`, `RandomDiffusionSteps`) to simplify implementation while still building the core U-Net from scratch.
*   **End-to-End PyTorch Workflow:** Practiced the full cycle: data loading/transformation, model definition, training loop, and inference/generation.

---

*Day 80 of #100DaysOfDataScience dives into the exciting realm of generative AI, building a conditional diffusion model from the ground up with PyTorch. A challenging but incredibly rewarding step! - Vatsal Parikh*
